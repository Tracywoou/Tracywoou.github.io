<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tracywoo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tracywoo.cn/"/>
  <updated>2021-07-29T01:03:54.318Z</updated>
  <id>http://tracywoo.cn/</id>
  
  <author>
    <name>Tracy</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>如何在word中打出好看的公式</title>
    <link href="http://tracywoo.cn/2021/07/29/%E5%A6%82%E4%BD%95%E5%9C%A8word%E4%B8%AD%E6%89%93%E5%87%BA%E5%A5%BD%E7%9C%8B%E7%9A%84%E5%85%AC%E5%BC%8F/"/>
    <id>http://tracywoo.cn/2021/07/29/%E5%A6%82%E4%BD%95%E5%9C%A8word%E4%B8%AD%E6%89%93%E5%87%BA%E5%A5%BD%E7%9C%8B%E7%9A%84%E5%85%AC%E5%BC%8F/</id>
    <published>2021-07-29T01:03:54.000Z</published>
    <updated>2021-07-29T01:03:54.318Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>python提取记事本文件中的数字绘制数字变化曲线</title>
    <link href="http://tracywoo.cn/2021/06/07/%E6%8F%90%E5%8F%96%E8%AE%B0%E4%BA%8B%E6%9C%AC%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%97%E7%BB%98%E5%88%B6%E6%95%B0%E5%AD%97%E5%8F%98%E5%8C%96%E6%9B%B2%E7%BA%BF/"/>
    <id>http://tracywoo.cn/2021/06/07/%E6%8F%90%E5%8F%96%E8%AE%B0%E4%BA%8B%E6%9C%AC%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%97%E7%BB%98%E5%88%B6%E6%95%B0%E5%AD%97%E5%8F%98%E5%8C%96%E6%9B%B2%E7%BA%BF/</id>
    <published>2021-06-07T10:38:16.000Z</published>
    <updated>2021-06-07T14:27:08.588Z</updated>
    
    <content type="html"><![CDATA[<h1 id="python读取记事本文件"><a href="#python读取记事本文件" class="headerlink" title="python读取记事本文件"></a>python读取记事本文件</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">filename = <span class="string">"D:\\OneDrive\\code\\aba\\output\\alarm\\v5\\sample10000_gen500_pop100_K2_0\\info.txt"</span></span><br><span class="line">f = open(filename, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">  ...: <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  ...:     line = f.readline()</span><br><span class="line">  ...:     <span class="keyword">if</span> line:</span><br><span class="line">  ...:         <span class="keyword">print</span> (line)</span><br><span class="line">  ...:     <span class="keyword">else</span>:</span><br><span class="line">  ...:         <span class="keyword">break</span></span><br><span class="line">  ...: f.close()</span><br></pre></td></tr></table></figure><h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">string = <span class="string">"gen 70: best score: -106886.61585569178, min sparsity: , true score: -106445.6497111178"</span></span><br><span class="line">all_score = re.findall(<span class="string">r"-\d+\.?\d*"</span>,string)</span><br><span class="line">x_gen = float(all_score[<span class="number">0</span>])</span><br><span class="line">true_score = float(all_score[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;python读取记事本文件&quot;&gt;&lt;a href=&quot;#python读取记事本文件&quot; class=&quot;headerlink&quot; title=&quot;python读取记事本文件&quot;&gt;&lt;/a&gt;python读取记事本文件&lt;/h1&gt;&lt;figure class=&quot;highlight pyth
      
    
    </summary>
    
    
    
      <category term="others" scheme="http://tracywoo.cn/tags/others/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://tracywoo.cn/2021/02/25/not%20yet/Wordvec2%20%20BERT/"/>
    <id>http://tracywoo.cn/2021/02/25/not%20yet/Wordvec2%20%20BERT/</id>
    <published>2021-02-24T19:51:02.124Z</published>
    <updated>2020-11-27T01:56:46.802Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Wordvec2-BERT"><a href="#Wordvec2-BERT" class="headerlink" title="Wordvec2 | BERT"></a>Wordvec2 | BERT</h1><h2 id="自然语言处理预训练模型"><a href="#自然语言处理预训练模型" class="headerlink" title="自然语言处理预训练模型"></a>自然语言处理预训练模型</h2><p>使语言建模和其他学习问题变得困难的一个基本问题是维数的诅咒。在人们想要对许多离散的随机变量（例如句子中的单词或数据挖掘任务中的离散属性）之间的联合分布建模时，这一点尤其明显。</p><p>举个例子，假如我们有10000个单词的词汇表，我们要对它们进行离散表示，这样用one-hot 编码整个词汇表就需要10000*10000的矩阵，而one-hot编码矩阵存在很多“0”值，显然浪费了绝大部分的内存空间。为了解决维度诅咒带来的问题，人们开始使用低维度的向量空间来表示单词，从而减少运算资源的损耗，这也是预训练模型思想的开端。</p><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>Skip-gram模型是Yoshua Bengio等人[1]提出的经典Word2Vec模型之一。Word2Vec模型对NLP任务的效果有显著的提升，并且能够利用更长的上下文。</p><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>2018年Jacob Devlin等人[2]提出预训练模型BERT（Bidirectional Encoder Representations from Transformers）。</p><p>BERT被设计为通过在所有层的双向上下文上共同进行条件化来预训练未标记文本的深层双向表示。我们可以在仅一个附加输出层的情况下对经过预训练的BERT模型进行微调，以创建适用于各种任务（例如问题解答和语言推断）的最新模型，进而减少了对NLP任务精心设计特定体系结构的需求。BERT是第一个<strong>基于微调</strong>的表示模型，可在一系列句子级和字符级任务上实现最先进的性能，优于许多特定于任务的体系结构。</p><p>通俗易懂来讲就是我们只需要把BERT当成一个深层次的Word2Vec预训练模型，对于一些特定的任务，我们只需要在BERT之后下接一些网络结构就可以出色地完成这些任务。</p><p>另外，2018年底提出的BERT推动了11项NLP任务的发展。BERT的结构是来自Transformers模型的Encoder，Transformers如图所示。我们从图中可以看到Transformer的内部结构都是由Ashish Vaswani 等人[3]提出的Self-Attention Layer和Layer Normalization的堆叠而产生。</p><h2 id="Self-Attention-Layer原理"><a href="#Self-Attention-Layer原理" class="headerlink" title="Self-Attention Layer原理"></a>Self-Attention Layer原理</h2><p><strong>1) Self-Attention Layer的出现原因</strong></p><p>为了解决RNN、LSTM等常用于处理序列化数据的网络结构无法在GPU中并行加速计算的问题。</p><p><strong>2) Self-Attention Layer的输入</strong></p><p>如图8.2所示：将输入的Input转化成token embedding + segment embedding +position embedding。因为有时候训练样本是由两句话组成，因此“[CLS]”是用来分类输入的两句话是否有上下文关系，而“[SEP]”则是用以分开两句话的标志符。其中，因为这里的Input是英文单词，所以在灌入模型之前，需要用BERT源码的Tokenization工具对每一个单词进行分词，分词后的形式如图 8.2中Input的“Playing”转换成“Play”+“# #ing”，因为英文词汇表是通过词根与词缀的组合来新增单词语义的，所以我们选择用分词方法可以减少整体的词汇表长度。如果是中文字符的话，输入就不需要分词，整段话的每一个字用“空格”隔开即可。</p><p>值得注意的是，模型是无法处理文本字符的，所以不管是英文还是中文，我们都需要通过预训练模型BERT自带的字典vocab.txt将每一个字或者单词转换成字典索引（即id）输入。</p><p>(1) segment embedding的目的：有些任务是两句话一起放入输入X，而segment便是用来区分这两句话的。在Input那里就是用“[SEP]”作为标志符号。而“[CLS]”用来分类输入的两句话是否有上下文关系。</p><p>(2) position embedding的目的：因为我们的网络结构没有RNN 或者LSTM，因此我们无法得到序列的位置信息，所以需要构建一个position embedding。构建position embedding有两种方法：BERT是初始化一个position embedding，然后通过训练将其学出来；而Transformer是通过制定规则来构建一个position embedding：使用正弦函数，位置维度对应曲线，而且方便序列之间的选对位置，使用正弦会比余弦好的原因是可以在训练过程中，将原本序列外拓成比原来序列还要长的序列，如公式（8.1）~（8.2）所示。<img src="//tracywoo.cn/2021/02/25/not yet/Wordvec2  BERT/F:%5Cimgdown%5Cv2-6f01f69b5693df76cebd8aec73c8ec4c_720w.jpg" alt="v2-6f01f69b5693df76cebd8aec73c8ec4c_720w"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Wordvec2-BERT&quot;&gt;&lt;a href=&quot;#Wordvec2-BERT&quot; class=&quot;headerlink&quot; title=&quot;Wordvec2 | BERT&quot;&gt;&lt;/a&gt;Wordvec2 | BERT&lt;/h1&gt;&lt;h2 id=&quot;自然语言处理预训练模型&quot;&gt;&lt;a h
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://tracywoo.cn/2021/02/25/not%20yet/Word2Vec%20%20Skip-gram/"/>
    <id>http://tracywoo.cn/2021/02/25/not%20yet/Word2Vec%20%20Skip-gram/</id>
    <published>2021-02-24T19:51:02.124Z</published>
    <updated>2020-11-27T01:29:15.764Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Word2Vec-Skip-gram"><a href="#Word2Vec-Skip-gram" class="headerlink" title="Word2Vec | Skip-gram"></a>Word2Vec | Skip-gram</h1><h2 id="文本表示"><a href="#文本表示" class="headerlink" title="文本表示"></a>文本表示</h2><p>计算机是无法直接处理文本信息的，所以，在我们构建神经网络之前，要对文本进行一定的处理。</p><p><strong>独热编码（one-hot encode）</strong>虽然能把所有文本用数字表示出来，但是表示文本的矩阵会非常的稀疏，极大地浪费了空间，而且这样一个矩阵放入神经网络训练也会耗费相当多的时间。</p><p>为此，Bengio等人[2]提出了<strong>词向量模型(Word2Vec)</strong>。词向量模型是一种将词的语义映射到向量空间的技术，说白了就是<em>用向量来表示词</em>，但是会比用独热编码用的空间小，而且词与词之间可以通过计算余弦相似度来看两个词的语义是否相近，显然King和Man两个单词语义更加接近，而且通过实验我们知道King-Man+Woman=Queen，验证了词向量模型的有效性。Word2Vec的示意图如图 4.54所示。</p><p><img src="//tracywoo.cn/2021/02/25/not yet/Word2Vec  Skip-gram/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201127091429404.png" alt="image-20201127091429404"></p><p>目前Word2Vec技术有好几种:CBOW、Skip-gram和GloVe以及2018年大火的BERT。Skip-gram是输入一个词，预测该词上下文的模型。</p><p><img src="//tracywoo.cn/2021/02/25/not yet/Word2Vec  Skip-gram/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201127091617231.png" alt="image-20201127091617231"></p><p>Skip-gram的具体训练过程如下，蓝色代表输入的词，框框代表滑动窗口，用来截取蓝色词的上下文，蓝色词的上下文作为输出，然后形成训练标本（Training Samples），这样我们就得到了{输入和输出}，将他们放入{输入层-隐藏层-输出层}的神经网络训练，我们就能得到Skip-gram模型。因为神经网络不能直接处理文本，因此所有的词都用one-hot encode表示。</p><p><img src="//tracywoo.cn/2021/02/25/not yet/Word2Vec  Skip-gram/F:%5Cimgdown%5Cv2-5af975f51adf94d793018dc8e1876bec_720w.jpg" alt="v2-5af975f51adf94d793018dc8e1876bec_720w"></p><p>Skip-gram的神经网络结构如图所示，隐藏层有300个神经元，输出层用softmax激励函数，通过我们提取的词与其相应的上下文去训练，得到相应的模型。通过Softmax激励函数，输出层每个神经元输出的是概率，加起来等于1。</p><p><img src="//tracywoo.cn/2021/02/25/not yet/Word2Vec  Skip-gram/F:%5Cimgdown%5Cv2-908055fe0361b40898b4423d04140b1c_720w.jpg" alt="v2-908055fe0361b40898b4423d04140b1c_720w"></p><p>但输出层并不是我们关心的，我们去掉模型的输出层，才是我们想要的词向量模型，我们通过隐藏层的权重来表示我们的词。</p><p>如图 所示，现在假设我们有10000个词，每个词用one-hot encode编码，每个词大小就是1*10000，现在我们想用300个特征去表示一个词，那么隐藏层的输入是10000，输出是300（即300个神经元），因此它的权值矩阵大小为10000 * 300。那么我们的词向量模型本质上就变成了矩阵相乘。</p><p><img src="//tracywoo.cn/2021/02/25/not yet/Word2Vec  Skip-gram/F:%5Cimgdown%5Cv2-efea42e3cfbfe0694a310ead96d2c518_720w.png" alt="v2-efea42e3cfbfe0694a310ead96d2c518_720w"></p><p>Keras自带了词向量层embedding layer，所以我们只要将文本处理好，就可以灌入这个层中即可，需要对文本进行预处理，生成符合embedding layer输入格式的词。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Word2Vec-Skip-gram&quot;&gt;&lt;a href=&quot;#Word2Vec-Skip-gram&quot; class=&quot;headerlink&quot; title=&quot;Word2Vec | Skip-gram&quot;&gt;&lt;/a&gt;Word2Vec | Skip-gram&lt;/h1&gt;&lt;h2 i
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://tracywoo.cn/2021/02/25/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    <id>http://tracywoo.cn/2021/02/25/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/</id>
    <published>2021-02-24T19:51:02.045Z</published>
    <updated>2020-07-26T08:35:14.662Z</updated>
    
    <content type="html"><![CDATA[<h1 id="启发式算法"><a href="#启发式算法" class="headerlink" title="启发式算法"></a>启发式算法</h1><p><strong>cost function：</strong>即在算法中的优化目标</p><p><strong>值域：</strong>可行域搜索的范围。</p><h1 id="随机搜索算法"><a href="#随机搜索算法" class="headerlink" title="随机搜索算法"></a>随机搜索算法</h1><p>最简单的优化算法，只适用于cost function在值域范围内没有任何变化规律的情况，找不到使得cost function下降的梯度和极小值点。步骤是在值域范围内生成足够多的可行解，分别计算每个可行解的代价，根据代价选择一个最小的可行解作为随机搜索的最优解。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/2ea1f135-bf6b-49b1-91cc-053c84fc1519/image.png" alt="img"></p><h1 id="爬山法"><a href="#爬山法" class="headerlink" title="爬山法"></a>爬山法</h1><p>知乎上的回答：</p><p>假想将解空间依照深度搜索序列的顺序为y轴，以解的权为x轴作图。</p><p><img src="https://pic1.zhimg.com/80/cd3cc0d96c509f338fbcc055fcddca2d_720w.jpg?source=1940ef5c" alt="img"></p><blockquote><p>我们可以认为得到一系列山峰与峡谷的剖面图。爬山算法就是在这个图上进行爬山，找到第一个山峰或者第一个符合要求高度的山峰就停止。具体来说，就是算法迭代时，每次用临近解空间内的更优解取代前解。</p><p>这一算法是简单的贪心算法，仅能得到局部最优解，往往不能得到全局最优解。<br>可见上图描述的搜索序列中，爬山算法会在第一个山峰处停下搜索，以局部最优解作为算法的结果。<br>这一算法是相对于各种全局最优算法在时间复杂度上的妥协，可以用于对最优情况不那么敏感、只需要取得可行解即可的情况。</p></blockquote><p>参考CSDN-<a href="https://blog.csdn.net/lvhao92/article/details/50826709" target="_blank" rel="noopener">https://blog.csdn.net/lvhao92/article/details/50826709</a></p><blockquote><p>成本函数抽象成了一座山（想象一下一个2维坐标系，横轴为变量，纵轴为成本函数，成本函数随着横轴的递增而上下起伏绵延不绝，好似一座山），某人可在山中一任意位置左右移动（取该函数中的一点），因此，随着某人水平方向的变化（变量的变化），这哥们的海拔高度也在变化（成本函数随着变量的变化而变化）。可惜，这哥们一心想去山的最底处。所以他总喜欢走下坡路，一旦发现各个方向再走都是上坡的时候，那这哥们认为他终于走到了山的最底处，他不再走了并返回此时他的位置。（该例子的成本函数仅和一个变量有关，但现实生活中，成本函数是和多个变量有关。道理也是一样，就好像这个哥们每次走路的时候有N个方向供他选择（N个变量））</p><p>明眼的人都能看出来，这哥们非常容易的会把局部最小值当成全局最小值。以为山的小凹谷就是整座山的最低处了（这座山绵延不绝，并不是两头连接大地的那种，是长度无限长的那种），too navie。</p><p>有什么办法呢？就是随机重复爬山法，让你每次初始位置都随机的多试几次。说不定还真能蒙到正确结果。</p><p>模拟退火算法：这哥们突然变聪明了，认为有时候退一步海阔天空，我有的时候稍微走点上坡路，指不定后面会有一个大下坡等着我呢，于是这哥们开始只要上坡路不是上升的特别离谱，他都会试着去尝试一下，走走看。不过，随着时间的流逝，这哥们开始越来越不愿意走上坡路，一开始可能这是上坡路还会去尝试着走一下呢。到后来，越来越不愿意去尝试。这个愿不愿意去尝试走上坡路的心态就跟刚出炉放在空气中的铁一样，随着时间的流逝而渐渐降温，渐渐冷却，渐渐退火。公式表示就是这样：<img src="https://img-blog.csdn.net/20160308144915487" alt="img">，如果新的成本函数降低了，当然欣然接受一开始，不多说。但是如果新的成本函数增高了，那么就开始考虑要不要试一下要不要走，一开始的时候，温度很高，高低成本之差显得很小，除了个温度接近为0，这个P值接近为1，一般程序当中都是用一个0和1之间的随机数与P值比较，如果比P小那么就尝试，如果比P大那么不尝试。所以很明显，一开始肯定是乐意尝试的，后来随着时间的增加，温度的下降，P值越来越小接近于0。因此，更加不愿意去尝试上坡路了。</p><p>这个方法的问题和爬山法其实差不多，每次结果可能都会不同，尝试着每次改下参数（初始温度和温度下降的速度）来试试。</p><p>遗传算法：这时候，想象成人类吧。人类的生存环境十分的残酷，只有优秀的一拨人才能活下来（优秀的人意为成本函数小的最优解），一代一代，每代之中会有变异（对既有解进行微小的，简单的，随机的改变）也会有交叉（选取最优解中的两个解，然后将他们按照某种方式进行结合）。很显然，变异和交叉会产生新的种群（会对成本函数产生或增或减的影响）。同样，这些新的种群有的能适应这个世界而存活下去，有的就消失在人类的进化长河里。正所谓物竞天择适者生存。真是残酷。</p></blockquote><h1 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h1><p>贪心算法，又名贪婪法，是寻找<strong>最优解问题</strong>的常用方法，这种方法模式一般将求解过程分成<strong>若干个步骤</strong>，但每个步骤都应用贪心原则，选取当前状态下<strong>最好/最优的选择</strong>（局部最有利的选择），并以此希望最后堆叠出的结果也是最好/最优的解。{看着这个名字，贪心，贪婪这两字的内在含义最为关键。这就好像一个贪婪的人，他事事都想要眼前看到最好的那个，看不到长远的东西，也不为最终的结果和将来着想，贪图眼前局部的利益最大化，有点走一步看一步的感觉。}</p><p><strong>贪婪法的基本步骤：</strong></p><p>步骤1：从某个初始解出发；<br>步骤2：采用迭代的过程，当可以向目标前进一步时，就根据局部最优策略，得到一部分解，缩小问题规模；<br>步骤3：将所有解综合起来。</p><h1 id="模拟退火"><a href="#模拟退火" class="headerlink" title="模拟退火"></a>模拟退火</h1><p>模拟热力学系统候选解状态的转换。</p><p><img src="//tracywoo.cn/2021/02/25/启发式算法/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200726161039318.png" alt="image-20200726161039318"></p><p><img src="//tracywoo.cn/2021/02/25/启发式算法/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200726163158381.png" alt="image-20200726163158381"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;启发式算法&quot;&gt;&lt;a href=&quot;#启发式算法&quot; class=&quot;headerlink&quot; title=&quot;启发式算法&quot;&gt;&lt;/a&gt;启发式算法&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;cost function：&lt;/strong&gt;即在算法中的优化目标&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;值
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://tracywoo.cn/2021/02/25/latex/"/>
    <id>http://tracywoo.cn/2021/02/25/latex/</id>
    <published>2021-02-24T19:51:02.013Z</published>
    <updated>2020-07-20T09:21:59.101Z</updated>
    
    <content type="html"><![CDATA[<h1 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h1><h2 id="英文格式"><a href="#英文格式" class="headerlink" title="英文格式"></a>英文格式</h2><p>在cmd命令行下进行tex文件的创建和编译。</p><p>mkdir: 创建目录</p><p>notepad test.tex 新建tex文件</p><p><code>\document{article}</code>论文格式的文档类</p><p>{}表示命令的参数</p><p>这里给一个案例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;article&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">Hello \LaTeX.</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p>编译时可以新建一个.bat文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">latex test.tex</span><br><span class="line">dvipdfmx test.dvi</span><br><span class="line">del *.aux *.dvi *.<span class="built_in">log</span></span><br></pre></td></tr></table></figure><p>最后的pdf结果为</p><p><img src="//tracywoo.cn/2021/02/25/latex/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200720171824711.png" alt="image-20200720171824711"></p><h2 id="中文"><a href="#中文" class="headerlink" title="中文"></a>中文</h2><p>若需要编译中文，需要选择存储的编码格式为utf-8。</p><p>\usepackage{ctex}中ctex为一个可以显示中文的宏包。</p><p>实例代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;article&#125;</span><br><span class="line"></span><br><span class="line">\usepackage&#123;ctex&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">你好， \LaTeX 。</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p>在进行编译的时候，可以新建一个.bat文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xelatex testc.tex</span><br><span class="line">del *.aux *.dvi *.<span class="built_in">log</span></span><br></pre></td></tr></table></figure><p>最后的编译结果为：</p><p><img src="//tracywoo.cn/2021/02/25/latex/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200720171852978.png" alt="image-20200720171852978"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;入门&quot;&gt;&lt;a href=&quot;#入门&quot; class=&quot;headerlink&quot; title=&quot;入门&quot;&gt;&lt;/a&gt;入门&lt;/h1&gt;&lt;h2 id=&quot;英文格式&quot;&gt;&lt;a href=&quot;#英文格式&quot; class=&quot;headerlink&quot; title=&quot;英文格式&quot;&gt;&lt;/a&gt;英文格式&lt;/h
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://tracywoo.cn/2021/02/25/EDA%E5%88%86%E5%B8%83%E4%BC%B0%E8%AE%A1%E7%AE%97%E6%B3%95/"/>
    <id>http://tracywoo.cn/2021/02/25/EDA%E5%88%86%E5%B8%83%E4%BC%B0%E8%AE%A1%E7%AE%97%E6%B3%95/</id>
    <published>2021-02-24T19:51:01.998Z</published>
    <updated>2020-08-04T08:02:55.158Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EDA分布估计算法"><a href="#EDA分布估计算法" class="headerlink" title="EDA分布估计算法"></a>EDA分布估计算法</h1><h2 id="解决问题的基本步骤"><a href="#解决问题的基本步骤" class="headerlink" title="解决问题的基本步骤"></a>解决问题的基本步骤</h2><ol><li>生成候选解；</li><li>评估生成解；</li><li>依据评估更新生成的候选解。</li></ol><h2 id="EDA的主要类型"><a href="#EDA的主要类型" class="headerlink" title="EDA的主要类型"></a>EDA的主要类型</h2><h3 id="基于种群的EDA"><a href="#基于种群的EDA" class="headerlink" title="基于种群的EDA"></a>基于种群的EDA</h3><p>根据所有可行解的均匀分布随机生成一个种群。每次迭代首先使用选择算子创建一个有希望的候选解的总体，这将优先选择高质量的解。任何流行的进化算法选择方法都可以使用，如截断或锦标赛选择。可以使用截断选择法，它选择总体中最上面的τ%成员。然后为选定的解建立一个概率模型。通过对构建模型编码的分布进行采样，可以创建新的解决方案。然后使用替换算子将新解合并到原始总体中。例如，在完全替换的情况下，候选解的整个原始总体将被新解替换。图1显示了基于总体的EDA的伪代码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">t ← 0</span><br><span class="line"></span><br><span class="line">generate population P(0) of random solutions</span><br><span class="line"></span><br><span class="line">while termination criteria not satisfied, repeat</span><br><span class="line"></span><br><span class="line">    evaluate all candidate solutions in P(t)</span><br><span class="line"></span><br><span class="line">    select promising solutions S(t) f r o m P(t)</span><br><span class="line"></span><br><span class="line">    build a probabilistic model M(t) f o r S(t)</span><br><span class="line"></span><br><span class="line">    generate new solutions O(t) by sampling M(t)</span><br><span class="line"></span><br><span class="line">    create P(t + 1) by combining O(t) a n d P(t)</span><br><span class="line"></span><br><span class="line">    t ← t + 1</span><br></pre></td></tr></table></figure><h3 id="增量EDA"><a href="#增量EDA" class="headerlink" title="增量EDA"></a>增量EDA</h3><p>在增量EDAs中，候选解的总体被概率模型完全取代。对模型进行初始化，以便对所有可容许解的均匀分布进行编码。然后，通过重复以下过程来逐步更新模型：（1）从当前模型中抽样几个候选解决方案；（2）基于对这些候选解决方案的评估及其比较对模型进行改进。增量EDA的伪代码如图2所示。</p><p>增量EDA一次通常只生成几个候选解决方案，而基于种群的EDA通常使用大量候选解决方案，从头开始构建每个模型。尽管如此，很容易看出这两种方法本质上是相同的，因为即使基于种群的eda也可以以增量的方式重新制定。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">t ← 0</span><br><span class="line"></span><br><span class="line">initialize model M(0) to represent the uniform distribution over admissible solutions</span><br><span class="line"></span><br><span class="line">while termination criteria not satisfied, repeat</span><br><span class="line"></span><br><span class="line">    generate population P(t) of candidate solutions by sampling M(t)</span><br><span class="line"></span><br><span class="line">    evaluate all candidate solutions in P(t)</span><br><span class="line"></span><br><span class="line">    create new model M(t + 1) by adjusting M(t) according to evaluated P(t)</span><br><span class="line"></span><br><span class="line">    t ← t + 1</span><br></pre></td></tr></table></figure><p>因此，EDA的主要组成部分包括</p><p>（1）用于选择有希望的解决方案的选择运算符，</p><p>（2）用于建模和采样的假定概率模型类，</p><p>（3）为选定的解决方案学习概率模型的过程，</p><p>（4）对已构建的概率模型进行抽样的过程，</p><p>（5）一个用于合并新旧候选解总体的替换算子。</p><p>学习一个概率模型的过程通常需要两个子组件：一个是从假设类中评估概率模型的度量，另一个是根据所使用的度量选择特定模型的搜索过程。EDA的区别主要在于概率模型的类别以及用于评估候选模型和搜索良好模型的程序。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;EDA分布估计算法&quot;&gt;&lt;a href=&quot;#EDA分布估计算法&quot; class=&quot;headerlink&quot; title=&quot;EDA分布估计算法&quot;&gt;&lt;/a&gt;EDA分布估计算法&lt;/h1&gt;&lt;h2 id=&quot;解决问题的基本步骤&quot;&gt;&lt;a href=&quot;#解决问题的基本步骤&quot; class
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://tracywoo.cn/2021/02/25/-TSP%E9%97%AE%E9%A2%98/"/>
    <id>http://tracywoo.cn/2021/02/25/-TSP%E9%97%AE%E9%A2%98/</id>
    <published>2021-02-24T19:51:01.982Z</published>
    <updated>2020-08-04T09:28:36.547Z</updated>
    
    <content type="html"><![CDATA[<p>解决TSP问题</p><h1 id="TSP问题"><a href="#TSP问题" class="headerlink" title="TSP问题"></a>TSP问题</h1><p>Traveling Salesman Problem - 旅行商问题，也被称为货郎担问题。假设有一个商人需要拜访n个城市，他必须选择要走的路径，路径的限制是每个城市只能拜访一次，并且最后需要回到出发的城市，路径的选择目标是要求找到所有路径的最小值。</p><h1 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h1><p>把解决问题的形式看成一个函数，则需要找到最值点，而下面介绍的算法都带有一定的随机性，达不到全局最优解的原因是他们陷入<strong>局部最优解</strong>的凹槽中无法解脱出来。由于解决TSP问题，因此需要找到谷底，即不断地下山。</p><h2 id="贪心法"><a href="#贪心法" class="headerlink" title="贪心法"></a>贪心法</h2><p>只着眼于眼前，目光短浅的选取下山方向。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. 随机构造初始解x0</span><br><span class="line">2.</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;解决TSP问题&lt;/p&gt;
&lt;h1 id=&quot;TSP问题&quot;&gt;&lt;a href=&quot;#TSP问题&quot; class=&quot;headerlink&quot; title=&quot;TSP问题&quot;&gt;&lt;/a&gt;TSP问题&lt;/h1&gt;&lt;p&gt;Traveling Salesman Problem - 旅行商问题，也被称为货郎担问
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://tracywoo.cn/2021/02/25/DE(%E5%B7%AE%E5%88%86%E6%BC%94%E5%8C%96%E7%AE%97%E6%B3%95)/"/>
    <id>http://tracywoo.cn/2021/02/25/DE(%E5%B7%AE%E5%88%86%E6%BC%94%E5%8C%96%E7%AE%97%E6%B3%95)/</id>
    <published>2021-02-24T19:51:01.982Z</published>
    <updated>2020-07-18T08:13:37.552Z</updated>
    
    <content type="html"><![CDATA[<p>算法族。</p><h1 id="DE-差分演化算法"><a href="#DE-差分演化算法" class="headerlink" title="DE(差分演化算法)"></a>DE(差分演化算法)</h1><p>用于求解实数优化问题，求解得到最小值的点。许多问题是不可微分的、不连续的、非线性的，使用经典算法难以得到好的解。因此需要使用启发式的方法来进行问题的求解。DE 可以用来求一个近似最优解。</p><h2 id="基本框架"><a href="#基本框架" class="headerlink" title="基本框架"></a>基本框架</h2><p>step1:初始化群体，不同搜索空间会有上界和下界，在范围内随机初始化。</p><p>step2: 当未满足迭代停止条件（一般是最大迭代次数）时不断：</p><p>① 变异：随机选择三个不同个体（作为父亲），对三个父代进行加权的差分。</p><p><img src="//tracywoo.cn/2021/02/25/DE(差分演化算法)/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200718145217428.png" alt="image-20200718145217428"></p><p>F的取值在0-2之间。</p><p>物理意义：</p><p><img src="//tracywoo.cn/2021/02/25/DE(差分演化算法)/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200718145849092.png" alt="image-20200718145849092"></p><p>② 重组：父亲和母亲互相交换一部分信息。</p><p><img src="//tracywoo.cn/2021/02/25/DE(差分演化算法)/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200718145335264.png" alt="image-20200718145335264"></p><p>v是变异以后得到的个体，CR是设置的阈值。随机产生随机数r，若r&gt;CR，则该位取父亲的值，定义了Irand来保证至少有一位来自于变异的个体。按位进行操作。</p><p>物理意义：</p><p><img src="//tracywoo.cn/2021/02/25/DE(差分演化算法)/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200718150008713.png" alt="image-20200718150008713"></p><p>③ 选择： 对于每一个后代计算函数值，若比父亲的值好则进行替换，否则保留父亲。</p><h2 id="相关记号"><a href="#相关记号" class="headerlink" title="相关记号"></a>相关记号</h2><p>N: 种群大小（大于4），一般设置为5*n-10*n之间的倍数。</p><p>D: 求解问题包含D个实数。</p><p>x: 决策变量，x(i,G)中i表示第i个个体，G表示当前迭代次数，是一个D维的向量。</p><p>F: 一般设置为[0,2]之间的实数，通常取0.5.</p><p>一个群体是一个矩阵。</p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p>收敛速度快；</p><p>擅长求解多变量的函数优化问题；</p><p>操作简单，容易实现。（20-30行代码）</p><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>算法后期个体间差异逐渐缩小，收敛速度慢，容易陷入局部最优。</p><p>控制参数和学习策略在算法性能影响有着重要的影响（F、CR）。</p><p>没有利用个体的先验性息，需要过多的迭代才能搜索到局部最优。</p><p><img src="//tracywoo.cn/2021/02/25/DE(差分演化算法)/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200718150438862.png" alt="image-20200718150438862"></p><p><img src="//tracywoo.cn/2021/02/25/DE(差分演化算法)/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200718150954639.png" alt="image-20200718150954639"></p><p><img src="//tracywoo.cn/2021/02/25/DE(差分演化算法)/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200718151101807.png" alt="image-20200718151101807"></p><h1 id="PSO-粒子群算法"><a href="#PSO-粒子群算法" class="headerlink" title="PSO(粒子群算法)"></a>PSO(粒子群算法)</h1><p>Partical Swarm Optimization - 模仿社会行为（群体行为）的动态交互。1995年由Kennedy&amp;Eberhart提出，是个体行为和社会行为的模仿。</p><p>群体行为智能模式：</p><p><strong>Separation:</strong> 避免群体之间过于紧密。</p><p><strong>Alignment:</strong> 群体倾向于朝一个方向移动。</p><p><strong>Cohesion:</strong> 小群体中心的聚集行为。</p><p>使用了粒子或智能体在搜索空间中进行Swarm移动来搜寻最优解。模仿鸟/鱼群移动到全局最优解。每个粒子会记录个体的历史和群体的历史。</p><p> 每个粒子会根据它的当前位置、当前搜索方向、当前位置和个人最优的距离、当前位置和全局最优的距离进行位置的调整。</p><p><strong>neighborhood</strong>：以一个点为中心的园。</p><h2 id="算法参数"><a href="#算法参数" class="headerlink" title="算法参数"></a>算法参数</h2><p>A：种群的代理</p><p>pi：代理ai在搜索空间中的位置</p><p>f：目标函数</p><p>vi：ai的方向</p><p>V(ai)：ai的邻居。</p><p>population size：一般设置10-50中的数（凭经验）。</p><h2 id="算法迭代过程"><a href="#算法迭代过程" class="headerlink" title="算法迭代过程"></a>算法迭代过程</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[x*] = PSO()</span><br><span class="line"><span class="number">1.</span> P = Particle_Initialization();</span><br><span class="line"><span class="number">2.</span> For <span class="built_in">i</span>=<span class="number">1</span> to it_max <span class="comment">% 迭代停止条件</span></span><br><span class="line"><span class="number">3.</span> For each particle p in P do</span><br><span class="line"><span class="number">4.</span> fp = f(p); <span class="comment">% 计算函数值</span></span><br><span class="line"><span class="number">5.</span> If fp is better than f(pBest) <span class="comment">% 若优于历史最优点</span></span><br><span class="line"><span class="number">6.</span> pBest = p; <span class="comment">% 更新粒子的最优点</span></span><br><span class="line"><span class="number">7.</span> End</span><br><span class="line"><span class="number">8.</span> End</span><br><span class="line"><span class="number">9.</span> gBest = best p in P; <span class="comment">% 更新全局最优点</span></span><br><span class="line"><span class="number">10.</span>For each particle p in P do <span class="comment">% 对于群体里的每个粒子执行</span></span><br><span class="line"><span class="number">11.</span><span class="comment">% 更新粒子的速度的值</span></span><br><span class="line"><span class="number">12.</span> v = v + c1*<span class="built_in">rand</span>*(pBest – p) + c2*<span class="built_in">rand</span>*(gBest – p);</span><br><span class="line"><span class="number">13.</span> <span class="comment">% 更新粒子的位置</span></span><br><span class="line"><span class="number">14.</span>p = p + v;</span><br><span class="line"><span class="number">15.</span> End</span><br><span class="line"><span class="number">16.</span> End</span><br></pre></td></tr></table></figure><p>p：粒子的当前速位置。通过p = p + v进行更新。</p><p>v：粒子的当前速度，粒子对于局部最优和全局最优进行学习，个体经验+群体经验。</p><p>c1，c2：控制参数，一般取和为4.</p><p>pbest：个体最好值。</p><h2 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h2><p>易于实现，与梯度无关，参数较少，有效的全局搜索方法。</p><h2 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h2><p>收敛速度过快时，容易陷入局部最优。在后期，算法的收敛速度缓慢。</p><h1 id="CMA-ES-协方差自适应调整的进化策略"><a href="#CMA-ES-协方差自适应调整的进化策略" class="headerlink" title="CMA/ES(协方差自适应调整的进化策略)"></a>CMA/ES(协方差自适应调整的进化策略)</h1><p><img src="//tracywoo.cn/2021/02/25/DE(差分演化算法)/C:%5CUsers%5C11647%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200718161259340.png" alt="image-20200718161259340"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;算法族。&lt;/p&gt;
&lt;h1 id=&quot;DE-差分演化算法&quot;&gt;&lt;a href=&quot;#DE-差分演化算法&quot; class=&quot;headerlink&quot; title=&quot;DE(差分演化算法)&quot;&gt;&lt;/a&gt;DE(差分演化算法)&lt;/h1&gt;&lt;p&gt;用于求解实数优化问题，求解得到最小值的点。许多问题是不可
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Git&amp;Githubub_eeerinzhang bilibili</title>
    <link href="http://tracywoo.cn/2020/07/13/Git-Githubub-eeerinzhang-bilibili/"/>
    <id>http://tracywoo.cn/2020/07/13/Git-Githubub-eeerinzhang-bilibili/</id>
    <published>2020-07-13T06:57:01.000Z</published>
    <updated>2020-07-13T12:28:44.881Z</updated>
    
    <content type="html"><![CDATA[<p>Git: Version control system</p><p>让不同的人在不同的地点在不同的时间线编辑file，可以有不同的Branch，通过Merge把大家的file合并。</p><p>Github: Software/ Tool for Git</p><p>Repository/ Git project: 保存文件夹所有的编辑历史。</p><p>Commit: 类似于文件的截图。</p><p>Branch: Master branch, Master branch的分支。</p><h1 id="Git使用"><a href="#Git使用" class="headerlink" title="Git使用"></a>Git使用</h1><p>git主页下载Git软件。下载完成后，输入<code>git --version</code>，若出现版本则安装成功。</p><p>Initialize  a git repository, <code>git init</code></p><p>配置user name, user email</p><p><img src="//tracywoo.cn/2020/07/13/Git-Githubub-eeerinzhang-bilibili/git1.png" alt="1"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Git: Version control system&lt;/p&gt;
&lt;p&gt;让不同的人在不同的地点在不同的时间线编辑file，可以有不同的Branch，通过Merge把大家的file合并。&lt;/p&gt;
&lt;p&gt;Github: Software/ Tool for Git&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>GAN_生成对抗网络</title>
    <link href="http://tracywoo.cn/2020/07/12/GAN-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    <id>http://tracywoo.cn/2020/07/12/GAN-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</id>
    <published>2020-07-12T01:09:07.000Z</published>
    <updated>2020-07-12T06:26:40.534Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GAN的基本思想"><a href="#GAN的基本思想" class="headerlink" title="GAN的基本思想"></a>GAN的基本思想</h1><p>图像生成：向量-&gt;图像。</p><p>条件生成：控制输出图像的类型。</p><p><strong>Generator：</strong> 向量-&gt;生成器-&gt;图像</p><p><strong>Discriminator：</strong>图像-&gt;判别器-&gt;分数</p><p>在GAN的训练过程中，通过生成器和判别器的拮抗作用进行物竞天择。</p><img src="//tracywoo.cn/2020/07/12/GAN-生成对抗网络/basic.png" alt="bi" style="zoom:67%;"><p>可以把生成器比做成学生，判别器作为老师，学生给老师交作业，通过老师的修改和反馈不断修改自己的作业，直到交出一份完美的答卷。</p><p>算法：</p><img src="//tracywoo.cn/2020/07/12/GAN-生成对抗网络/al2.png" alt="al" style="zoom:67%;"><p>在每次迭代中，从数据库中采样一些样本。</p><p>s1: 固定生成器G，更新判别器D；</p><p>s2: 固定判别器D，更新生成器G。</p><p>生成器需要学着去愚弄判别器。</p><img src="//tracywoo.cn/2020/07/12/GAN-生成对抗网络/AL3.png" alt="AL" style="zoom:67%;"><p>可以把GAN看成结构学习算法，结构学习中的数据往往不是一个简单的数值，如在机器翻译中得到的结果是一段话，在图像生成中得到的结果是一张图像。往往得到的结果都是未出现过的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;GAN的基本思想&quot;&gt;&lt;a href=&quot;#GAN的基本思想&quot; class=&quot;headerlink&quot; title=&quot;GAN的基本思想&quot;&gt;&lt;/a&gt;GAN的基本思想&lt;/h1&gt;&lt;p&gt;图像生成：向量-&amp;gt;图像。&lt;/p&gt;
&lt;p&gt;条件生成：控制输出图像的类型。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
    
      <category term="ML" scheme="http://tracywoo.cn/categories/ML/"/>
    
    
  </entry>
  
  <entry>
    <title>MOEA测试函数</title>
    <link href="http://tracywoo.cn/2020/07/12/MOEA%E6%B5%8B%E8%AF%95%E5%87%BD%E6%95%B0/"/>
    <id>http://tracywoo.cn/2020/07/12/MOEA%E6%B5%8B%E8%AF%95%E5%87%BD%E6%95%B0/</id>
    <published>2020-07-12T00:34:57.000Z</published>
    <updated>2020-07-12T01:00:31.565Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MOEA测试的基本问题"><a href="#MOEA测试的基本问题" class="headerlink" title="MOEA测试的基本问题"></a>MOEA测试的基本问题</h1><blockquote><p><strong>MOEA测试的具体内容</strong></p><p>多目标优化问题的一个测试函数或者一组测试函数，或一个实际应用问题可以构成对MOEA的测试。</p></blockquote><blockquote><p><strong>如何找到合适的MOEA测试函数</strong></p><p>从已知的MOEA文献或者他人提出和总结的构造测试函数的方法，或者为人熟知的实际问题中找到一个合适的MOEA测试。</p></blockquote><blockquote><p><strong>何时对MOEA进行测试</strong></p><p>在MOEA设计与实现的过程不断地对它进行测试，也可以在一个MOEA完全实现后再对它进行测试</p></blockquote><h1 id="选择MOP时需要考虑的特征"><a href="#选择MOP时需要考虑的特征" class="headerlink" title="选择MOP时需要考虑的特征"></a>选择MOP时需要考虑的特征</h1><ol><li>连续的或非连续的或离散的；</li><li>可导的或不可导的；</li><li>凸的或凹的；</li><li>函数的形态（单峰的，多峰的）；</li><li>数值函数或包含字母与数字的函数；</li><li>二次方的或非二次方的；</li><li>约束条件的类型（等式、不等式、线性的、非线性的）；</li><li>低维的或高维的（基因型、表现型）；</li><li>欺骗问题或非欺骗问题；</li><li>相对真实PF时候有偏好。</li></ol><h1 id="常见的测试函数集"><a href="#常见的测试函数集" class="headerlink" title="常见的测试函数集"></a>常见的测试函数集</h1><h2 id="ZDT"><a href="#ZDT" class="headerlink" title="ZDT"></a>ZDT</h2><p><img src="//tracywoo.cn/2020/07/12/MOEA测试函数/ZDT.png" alt="ZDT"></p><p>其他的测试函数还有DTLZ和WGF系列。（填坑）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;MOEA测试的基本问题&quot;&gt;&lt;a href=&quot;#MOEA测试的基本问题&quot; class=&quot;headerlink&quot; title=&quot;MOEA测试的基本问题&quot;&gt;&lt;/a&gt;MOEA测试的基本问题&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;MOEA测试的具体内容&lt;/
      
    
    </summary>
    
    
      <category term="MOEA" scheme="http://tracywoo.cn/categories/MOEA/"/>
    
    
  </entry>
  
  <entry>
    <title>Archer主题扩展使用</title>
    <link href="http://tracywoo.cn/2020/07/12/Archer%E4%B8%BB%E9%A2%98%E6%89%A9%E5%B1%95%E4%BD%BF%E7%94%A8/"/>
    <id>http://tracywoo.cn/2020/07/12/Archer%E4%B8%BB%E9%A2%98%E6%89%A9%E5%B1%95%E4%BD%BF%E7%94%A8/</id>
    <published>2020-07-12T00:16:27.000Z</published>
    <updated>2020-07-12T00:29:49.282Z</updated>
    
    <content type="html"><![CDATA[<h1 id="自定义文章页头图"><a href="#自定义文章页头图" class="headerlink" title="自定义文章页头图"></a>自定义文章页头图</h1><p>通过配置每篇文章(即.md文件)的头部的<code>header_image</code>字段来修改，如果不填写将采用主题配置中的<code>post_header_image</code>字段，如果<code>post_header_image</code>字段未填写则采用<code>site_header_image</code>字段。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: Tags</span><br><span class="line">date: 2013-12-24 23:29:53</span><br><span class="line">tags:</span><br><span class="line">- Foo</span><br><span class="line">- Bar</span><br><span class="line">- Baz</span><br><span class="line">header_image: /intro/post-bg.jpg</span><br><span class="line">---</span><br></pre></td></tr></table></figure><h1 id="将-Unsplash-随机图片作为头图"><a href="#将-Unsplash-随机图片作为头图" class="headerlink" title="将 Unsplash 随机图片作为头图"></a>将 Unsplash 随机图片作为头图</h1><p>API 参数页面：<a href="https://source.unsplash.com/" target="_blank" rel="noopener">https://source.unsplash.com/</a></p><p>在 archer 的配置文件中将头图设为上面的 API 提供的 URL，可以设置图片大小和范围等等，也可以使用自己的 Unsplash 的账户收藏的图片等等，这样每次访问时将会随机加载不同的头图。</p><h1 id="切换代码配色方案"><a href="#切换代码配色方案" class="headerlink" title="切换代码配色方案"></a>切换代码配色方案</h1><p>主题现在有两套代码配色：atom-one-dark 及 atom-one-light（v1.5.0 开始默认 atom-one-dark）。 配色取自 <a href="https://highlightjs.org/static/demo/" target="_blank" rel="noopener">hightlight</a></p><h2 id="切换方法"><a href="#切换方法" class="headerlink" title="切换方法"></a>切换方法</h2><ol><li>修改 <code>archer\source-src\scss\_variables.scss</code> 中的 <code>$code-theme</code>，可以更换为 <code>atom-one-dark</code> 或 <code>atom-one-light</code>。</li><li>在 archer 目录下执行 <code>npm install</code>，然后执行 <code>npm run build</code>。</li><li>再重新执行 <code>hexo g</code> 即可。</li></ol><h1 id="解决Hexo文章置顶功能"><a href="#解决Hexo文章置顶功能" class="headerlink" title="解决Hexo文章置顶功能"></a>解决Hexo文章置顶功能</h1><p><a href="https://www.jianshu.com/p/42a4efcdf8d7" target="_blank" rel="noopener">文章置顶功能</a></p><h1 id="设置文章版权信息"><a href="#设置文章版权信息" class="headerlink" title="设置文章版权信息"></a>设置文章版权信息</h1><p>修改主题配置文件中的 copyright 字段开启/关闭：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">copyright:</span><br><span class="line">  enable: true</span><br><span class="line">  # https://creativecommons.org/</span><br><span class="line">  license: &apos;本文采用&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc/4.0/&quot;&gt;知识共享署名-非商业性使用 4.0 国际许可协议&lt;/a&gt;进行许可&apos;</span><br></pre></td></tr></table></figure><p>默认显示作者，链接，日期以及版权说明，版权说明可通过 copyright.lincese 自定义。</p><p><strong>copyright.lincese 的值可以是 HTML</strong></p><p>当文章版权信息开启时，可通过文章 Markdown 头部：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">copyright: false</span><br></pre></td></tr></table></figure><p>进行单篇文章版权信息的关闭。</p><hr><p>special thanks to <a href="https://github.com/ahonn/hexo-theme-even/wiki/设置文章版权信息" target="_blank" rel="noopener">hexo-theme-even</a>，这部分说明直接我是直接抄过来的..</p><h1 id="启用-Algolia-搜索"><a href="#启用-Algolia-搜索" class="headerlink" title="启用 Algolia 搜索"></a>启用 Algolia 搜索</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>有几种方案可选：</p><ol><li>选用 <code>hexo-content-json</code> 类的 hexo 插件，生成一个包含所有文章内容的 xml 或 json，这样的好处是配置步骤少，但是会下载一个 json 或 xml，体积与所有文章的内容成正比，如果文章数量较多的话要下载的体积就会比较大，文章数量少的话又没啥必要。</li><li>选用 <code>hexo-content-json</code> 类的 hexo 插件，生成一个只包含文章题目的 xml 或 json，这样文件的体积会小很多，hexo-theme-yilia 是采用的这种方案，并且将 <code>#</code> 搜索 tag 的功能也加上了，但是 archer 已经有了 tag 分类，这样做就仅仅是搜索题目了，用处也不大。</li><li>选用第三方的搜索，具有搜索所有文章内容的功能，也是目前 hexo 主题届的老大 hexo-theme-next 采用的方案，最后选定了这种。</li></ol><p>PS: 感谢 Algolia &amp; hexo-theme-next</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="注册帐号-可以用-Github-登录"><a href="#注册帐号-可以用-Github-登录" class="headerlink" title="注册帐号(可以用 Github 登录)"></a>注册帐号(可以用 Github 登录)</h3><p>前往 Algolia <a href="https://www.algolia.com/" target="_blank" rel="noopener">注册页面</a>，注册一个新账户。 可以使用 GitHub 或者 Google 账户直接登录，注册后的 14 天内拥有所有功能（包括收费类别的）。之后若未续费会自动降级为免费账户，免费账户 总共有 10,000 条记录，每月有 100,000 的可以操作数。注册完成后，创建一个新的 Index，这个 Index 将在后面使用。</p><p><img src="https://user-images.githubusercontent.com/12322740/40921716-d512bae6-6842-11e8-804e-53a8e71206ab.png" alt="image"></p><h3 id="安装-Algolia"><a href="#安装-Algolia" class="headerlink" title="安装 Algolia"></a>安装 Algolia</h3><p>Index 创建完成后，此时这个 Index 里未包含任何数据。 接下来需要安装 <a href="https://github.com/oncletom/hexo-algolia" target="_blank" rel="noopener">Hexo Algolia</a> 扩展， 这个扩展的功能是搜集站点的内容并通过 API 发送给 Algolia。前往站点根目录，执行命令安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-algolia</span><br></pre></td></tr></table></figure><h3 id="获取-keys"><a href="#获取-keys" class="headerlink" title="获取 keys"></a>获取 keys</h3><p>在 Algolia 服务站点上找到需要使用的一些配置的值，包括 ApplicationID、Search-Only API Key、 Admin API Key。注意，Admin API Key 需要保密保存。点击ALL API KEYS 找到新建INDEX对应的key， 编辑权限，在弹出框中找到ACL选择勾选Add records, Delete records, List indices, Delete index权限，点击update更新。</p><h1 id="启用-Latex-支持"><a href="#启用-Latex-支持" class="headerlink" title="启用 Latex 支持"></a>启用 Latex 支持</h1><p>事实上启用对 Latex 的支持是 Hexo 的一项功能，而与 archer 无关。 可以参照这两篇文章来开启对 Latex 的支持：</p><ol><li><a href="https://nathaniel.blog/tutorials/make-hexo-support-math-again/" target="_blank" rel="noopener">Make Hexo Support Math Again</a></li><li><a href="http://2wildkids.com/2016/10/06/如何处理Hexo和MathJax的兼容问题/#小结" target="_blank" rel="noopener">如何处理Hexo和MathJax的兼容问题</a></li></ol><h1 id="英文界面"><a href="#英文界面" class="headerlink" title="英文界面"></a>英文界面</h1><p>将 Hexo 配置中的 <code>language</code> 修改为 <code>language: en</code> 即可切换为英文界面。 暂时只支持英文，其他会默认中文。</p><p>也可自行修改 <code>language</code> 文件夹下的 <code>en.yml</code> 来自定义英文界面。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;自定义文章页头图&quot;&gt;&lt;a href=&quot;#自定义文章页头图&quot; class=&quot;headerlink&quot; title=&quot;自定义文章页头图&quot;&gt;&lt;/a&gt;自定义文章页头图&lt;/h1&gt;&lt;p&gt;通过配置每篇文章(即.md文件)的头部的&lt;code&gt;header_image&lt;/code&gt;字
      
    
    </summary>
    
    
      <category term="博客" scheme="http://tracywoo.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
  </entry>
  
  <entry>
    <title>多目标优化算法评价指标</title>
    <link href="http://tracywoo.cn/2020/07/11/%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    <id>http://tracywoo.cn/2020/07/11/%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</id>
    <published>2020-07-11T02:33:47.000Z</published>
    <updated>2020-07-22T03:07:53.990Z</updated>
    
    <content type="html"><![CDATA[<p>对于MOEA的评价主要考虑两个指标：MOEA的效果（它所求得的Pareto最优解集的质量，主要是MOEA的收敛效果和分布效果）和MOEA的效率（CPU时间以及占用的空间资源）。在对于算法进行评价时，通常使用benchmark test problem(有已知解的问题)作为测试用例。</p><h1 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h1><p>通常可以采用实验的方法对于MOEA的性能进行测试和分析。一个好的实验过程包含下列步骤：</p><p><strong>① 确定实验的目的；</strong></p><blockquote><p>针对相同的测试问题对不同的MOEA进行比较实验；</p><p>针对某个MOEA关于它的性能特征进行实验。</p></blockquote><p>所采用的MOEA应具有代表性，如NSGAⅡ、SPEA2、MOEA/D等。</p><p><strong>② 选取合适的MOEA性能评价工具（或方法）；</strong></p><p>MOEA的主要指标为</p><blockquote><p><strong>所求解集的质量</strong></p><p>对于benchmark测试问题，可以比较所求解集和最优解集的偏差。</p><p><u>对于没有已知最优解的优化问题，可以采用趋近度评价方法，比较所求解集和参照解集的最小距离来表示（历代非支配集并集的非支配集）。</u></p></blockquote><blockquote><p><strong>计算效率</strong></p><p>MOEA运行的CPU时间</p><p>关键操作的迭代次数</p><p>“质量-时间”曲线，一般情况下，解集的质量回随着时间的增加而变好。</p></blockquote><blockquote><p><strong>鲁棒性</strong></p><p>如果一个MOEA只对于某一个具体的问题又很好的求解问题，那么这个MOEA称不上是鲁棒的，一个鲁棒的MOEA应当有比较广泛的应用领域，在求解问题的时候有比较好的稳定性。</p><p>求解问题特征的敏感性、对待数据质量的敏感性以及对于不同参数设置的敏感性也是衡量算法鲁棒性的重要指标。 </p></blockquote><p><strong>③ 选取具有代表性的测试用例；</strong></p><blockquote><p> <strong>测试问题中的参数</strong></p></blockquote><blockquote><p> <strong>MOEA自身的参数</strong></p><p>population size</p><p>mating restriction</p><p>fitness assignment</p><p>sharing mechanism</p><p>individual representation</p></blockquote><blockquote><p><strong>实验参数设置</strong></p><p>MOEA运行的软件和硬件条件，因此实验报告中需要说明使用的实验环境。</p></blockquote><p><strong>④ 实验及实验结果分析；</strong></p><blockquote><p>MOEA运行的过程中存在随机性，一般选取10~30次MOEA独立运行的平均结果。分析时，要重点分析不同参数对于MOEA性能的影响。要对于负结果进行分析。</p></blockquote><p><strong>⑤ 采取合适的方式（表/图）描述实验结果；</strong></p><h1 id="性能评价方法"><a href="#性能评价方法" class="headerlink" title="性能评价方法"></a>性能评价方法</h1><p>函数值应当在0~1之间；</p><p>期望的函数值应当是可知的；</p><p>评价曲线应当是迭代递增或者递减的；</p><p>评价函数适应于任意多个目标。</p><p>函数的计算复杂度不能太高。</p><blockquote><p>评价所求解集与真正Pareto最右面的趋近程度来评价MOEA的收敛性；</p><p>评价解集的分布性；</p><p>综合考虑解集的收敛性和分布性。</p></blockquote><h1 id="性能评估指标"><a href="#性能评估指标" class="headerlink" title="性能评估指标"></a>性能评估指标</h1><h2 id="runtime"><a href="#runtime" class="headerlink" title="runtime"></a>runtime</h2><p>程序运行时间。</p><h2 id="Set-Coverage（C-metric）"><a href="#Set-Coverage（C-metric）" class="headerlink" title="Set Coverage（C-metric）"></a>Set Coverage（C-metric）</h2><p>对于两个近似解集A和B，C(A,B)是B被A中至少一个解支配的解占B中解个数的百分比。C(A,B)=1表示B中所有的解都被A中至少一个解支配，即 B 的收敛性比 A 差。C(A,B)=0表示B中没有解被A中的解支配，即 B 的收敛性优于 A 。C(A,B)不一定等于1-C(B,A)。这样就很清晰了，下面给出公式：</p><p><img src="//tracywoo.cn/2020/07/11/多目标优化算法评价指标/CS.png" alt="cov"></p><p>因为C-metric 的计算是基于支配关系，所以其最大缺点是随着目标数的增多，PF 近似解集中的解均彼此互为非支配解，C-metric 无法度量收敛性。  </p><h2 id="GD（Generational-Distance-世代距离"><a href="#GD（Generational-Distance-世代距离" class="headerlink" title="GD（Generational Distance)世代距离"></a>GD（Generational Distance)世代距离</h2><p>该方法是 Van Veldhuizen 和 Lamont 两人于1998 年提出的，需要预先获得一组在真实 PF 上均匀采样的解集。该指标表示求出<strong>已知的最优边界PFknown和问题真正的最优边界PFtrue之间的间隔距离</strong>，该距离表示偏离真正边界的程度，值越大，偏离真正最优边界越远，收敛性越差。设 P* 为一组在真实 PF 上均匀采样的解集，S 是多目标进化算法求得的 PF 近似解集，则GD 定义如下所示。</p><p><img src="//tracywoo.cn/2020/07/11/多目标优化算法评价指标/GD2.png" alt="GD"></p><p>其中，dist(x, S) 表示S中的个体x 到 P* 上离其最近个体之间的欧氏距离，| S | 是集合 S 的基数。GD 值越小，表示 S具有越好的收敛性，越能逼近整个 PF。</p><h2 id="IGD"><a href="#IGD" class="headerlink" title="IGD"></a>IGD</h2><p>逆世代距离IGD是世代距离GD的逆向映射，它计算问题真正的Pareto最优解集PFtrue中的个体到算法求得的非支配解集PFknown的平均距离。具有较小IGD值的解集更好。它除了能反应解集的收敛性外，还可以很好得反应分布均匀性和广泛性。IGD值越小，多样性和收敛性越好。公式如下：<br><img src="//tracywoo.cn/2020/07/11/多目标优化算法评价指标/IGD.png" alt="IGD"></p><h2 id="HV"><a href="#HV" class="headerlink" title="HV"></a>HV</h2><p>根据Zitzler等人的说法（2003），Hyper Volume指标是唯一已知的一元指标，它可以由其超体积的单个值来评估一个解集的质量， 且是唯一已知的符合帕累托支配概念的指标。 超体积是评估近似解集的收敛性和多样性的综合指标（Zitzler和Thiele 1999）。 因此，给定在n个目标中包含m个点的集合S，解集S的超体积是由S中的至少一个点支配的目标空间的一部分的大小。相对于参考点计算S的超体积，该参考点在每个目标中的比S中的每个点更差（或等于）。 如图8所示，超体积值越大，就认为该解集越好。 超体积的一个主要优点（Zitzler和Thiele 1999; Zitzler等人，2003）是它能够以单个数字得到解与最优集合的接近程度，并在某种程度上得到目标空间上解的分布。缺点是计算消耗过大，且参考点选取对准确定有一定的影响。</p><p><img src="//tracywoo.cn/2020/07/11/多目标优化算法评价指标/HV.png" alt="hv"></p><h2 id="Spacing（空间评价）"><a href="#Spacing（空间评价）" class="headerlink" title="Spacing（空间评价）"></a>Spacing（空间评价）</h2><p>可衡量 PF 近似解集中个体在目标空间的分布情况。  </p><img src="//tracywoo.cn/2020/07/11/多目标优化算法评价指标/Spacing.png" alt="sp" style="zoom: 50%;"><img src="//tracywoo.cn/2020/07/11/多目标优化算法评价指标/s2.png" alt="s2" style="zoom:50%;">]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;对于MOEA的评价主要考虑两个指标：MOEA的效果（它所求得的Pareto最优解集的质量，主要是MOEA的收敛效果和分布效果）和MOEA的效率（CPU时间以及占用的空间资源）。在对于算法进行评价时，通常使用benchmark test problem(有已知解的问题)作为测
      
    
    </summary>
    
    
    
      <category term="MOEA" scheme="http://tracywoo.cn/tags/MOEA/"/>
    
  </entry>
  
  <entry>
    <title>platEMO_多目标优化工具箱</title>
    <link href="http://tracywoo.cn/2020/07/10/platEMO-%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E5%B7%A5%E5%85%B7%E7%AE%B1/"/>
    <id>http://tracywoo.cn/2020/07/10/platEMO-%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E5%B7%A5%E5%85%B7%E7%AE%B1/</id>
    <published>2020-07-10T13:14:11.000Z</published>
    <updated>2020-07-11T02:43:11.248Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>platEMO是Ye Tian等学者写的一款基于MATLAB的多目标优化工具。</p><p>这款工具主要具有以下的几个特点：<br>1.完全由MATLAB开发，不需要任何其它库。<br>2.用户可以显示各种图形，包括结果的pareto front，真实的pareto front等等。<br>3.强大友好的GUI，可以不用编辑任何代码，直接调用主函数main.m即可。<br>4.可以直接生成Excel或者LaTex。</p><h1 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h1><p><a href="https://github.com/BIMK/PlatEMO" target="_blank" rel="noopener">github</a></p><p>该平台使用的详细信息可以在platEMO中的manual.pdf中读取。</p><p><a href="http://bimk.ahu.edu.cn/index.php?s=/Index/Software/index.html" target="_blank" rel="noopener">实验室主页</a> </p><p>下载后，将matlab运行目录设置为该文件夹即可。</p><h1 id="文件夹结构"><a href="#文件夹结构" class="headerlink" title="文件夹结构"></a>文件夹结构</h1><p><img src="//tracywoo.cn/2020/07/10/platEMO-多目标优化工具箱/%E6%96%87%E4%BB%B6.png" alt="文件夹"></p><p><strong>Algorithm：</strong></p><p>包括现有的90个流行的MOEAs，包括遗传算法、差分进化、粒子群优化、模因算法、分布估计算法和基于代理模型的算法。其中大多数是2010年以后在顶级期刊上发表的代表性算法。</p><p><strong>Data：</strong></p><p>包括120多个benchmark MOPs。</p><p><strong>GUI：</strong></p><p>用于存储平台界面的源代码。</p><p><strong>Metrics：</strong></p><p>用于存储所有性能指标的源代码。</p><p><strong>Operators:</strong></p><p>用于存储所有算子的源代码。</p><p><strong>Problems:</strong></p><p>用于存储所有多目标优化问题的源代码。</p><p><strong>Public:</strong></p><p>用于存储所有类和实例的函数。</p><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><h2 id="命令行使用platEMO"><a href="#命令行使用platEMO" class="headerlink" title="命令行使用platEMO"></a>命令行使用platEMO</h2><p>用户可以通过调用带有输入参数的接口函数main（）来运行PlatEMO的命令模式。</p><p>如果main（）在没有任何输入参数的情况下被调用，则GUI模式将运行。</p><p>下表列出了main（）的所有可接受参数。注意，用户不需要分配所有参数，因为每个参数都有一个默认值。</p><p><img src="//tracywoo.cn/2020/07/10/platEMO-多目标优化工具箱/parameter.png" alt="参数"></p><p><strong>-algorithm.</strong> 要执行的MOEA的函数。<br><strong>-problem.</strong> 待解决的MOP。<br><strong>-N.</strong> MOEA的种群规模。注意，它被固定在某些MOEAs(例如moead .m)中的某些特定值上，因此这些MOEAs的实际种群大小可能并不完全等于这个参数。<br><strong>-M.</strong> MOP的目标数目。注意，在不可伸缩的MOPs(例如ZDT1.m)中，目标的数量是恒定的，因此这个参数对于这些MOPs是无效的。<br><strong>-D.</strong> MOP决策变量的个数。注意，在某些MOP中，决策变量的数量是常量或固定到某些特定整数上的(例如：ZDT5.m)，因此决策变量的实际数量可能并不完全等于这个参数。<br><strong>-evaluation.</strong> 函数评价的最大数目。<br><strong>-run.</strong> 运行数。如果用户希望为相同的算法、问题、M和D参数保存多个结果，则在每次运行时修改此参数，使结果的文件名不同。<br><strong>-save.</strong> 保存的种群。如果将该参数设置为0(默认值)，则会在终止后显示结果图;否则，在进化过程中获得的种群将保存在一个名为Data\algorithm\algorithm_problem_M_D_run.mat的文件中。例如：如果save为5并且evaluation是20000，评价数量为4000、8000、12000、16000、20000时得到的种群将被保存。<br><strong>-outputFcn.</strong> 每次生成后调用的函数，通常不需要修改。</p><p>例如：使用下面的命令在WFG1上运行RVEA，其种群大小为200和10个目标，最终结果将显示出来：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">main(<span class="string">'-algorithm'</span>,@RVEA,<span class="string">'-problem'</span>,@WFG1,<span class="string">'-N'</span>,<span class="number">200</span>,<span class="string">'-M'</span>,<span class="number">10</span>);</span><br></pre></td></tr></table></figure><p>使用以下命令在WFG2上运行KnEA，并在KnEA和WFG2中设置参数:</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">main(<span class="string">'-algorithm'</span>,&#123;@KnEA,<span class="number">0.4</span>&#125;,<span class="string">'-problem'</span>,&#123;@WFG2,<span class="number">6</span>&#125;);</span><br></pre></td></tr></table></figure><p>每个MOEA和MOP的具体参数可以在对应函数头部的注释中找到。使用以下命令在DTLZ5上运行AR-MOEA10次，最终保存种群：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> r = <span class="number">1</span> : <span class="number">10</span></span><br><span class="line">     main(<span class="string">'-algorithm'</span>,@ARMOEA,<span class="string">'-problem'</span>,@DTLZ5,<span class="string">'-save'</span>,<span class="number">1</span>, <span class="string">'-run'</span>,r);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="使用带GUI的platEMO"><a href="#使用带GUI的platEMO" class="headerlink" title="使用带GUI的platEMO"></a>使用带GUI的platEMO</h2><p>用户可以通过以下命令运行PlatEMO的GUI模式：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">main();</span><br></pre></td></tr></table></figure><p>然后在GUI上可以看到两个模块，即测试模块和实验模块。测试模块用于每次在MOP上执行一个MOEA，结果将以图的形式显示。实验模块将同时在几个MOPs上执行多个MOEAs。统计结果将列在表格中。</p><p><img src="//tracywoo.cn/2020/07/10/platEMO-多目标优化工具箱/STRU.png" alt="结构"></p><p>A: 选择执行的算法和多目标优化问题。</p><p>B: 设置所选MOEA和MOP的参数。每个参数的值应该是一个标量。注意这里的公共参数N,M,D和evaluation都视为MOP的参数。如果将参数设置为空，则该参数将等于其默认值。</p><p>C: 根据当前配置执行MOEA。</p><p>D: 显示光标移动到的区域B中的参数的介绍。</p><p>E: 显示优化过程中的当前种群。</p><p>F: 放大，缩小，平移或旋转E区域的轴。</p><p>G: 在新的标准MATLAB图中打开E区域的坐标轴，可以对坐标轴进行更多的操作，例如保存坐标轴。</p><p>H: 选择要显示在E区域轴线上的数据，包含种群的pareto front，种群的pareto集合，MOP的真实pareto front和任何性能度量的收敛轮廓。</p><img src="//tracywoo.cn/2020/07/10/platEMO-多目标优化工具箱/K.png" alt="H" style="zoom:50%;"><p>I: 控制优化过程，即，开始，暂停，停止，后退和前进。<br>J: 显示其中一个历史结果。<br>K: 显示结果的最终种群的性能度量值。<br>L: 显示执行的详细信息。<br>用户打开测试模块后，首先选择要在A区域执行的MOEA和MOP，并在B区域设置参数，然后按下区域C按钮执行算法。实时种群将显示在E区域的轴线上，用户可以使用I区域的按钮来控制优化过程。算法结束后，通过选择J区域的弹出菜单，可以重新显示所有的历史结果。</p><p><img src="//tracywoo.cn/2020/07/10/platEMO-多目标优化工具箱/TEST.png" alt="TEST"></p><p>PlatEMO实验模块界面如上图所示。各区域管制的职能如下：<br>Region A. 选择要执行的MOEA和MOP。<br>Region B.设置所选MOEA和MOP的参数。MOPs里的每个参数的值都可以是一个向量。因此，MOEAs可以在相同的MOP上以不同的设置执行。<br>Region C.设置每个文件中保存的种群数量。例如，如果种群大小为5，评估的数量为20000，评价数量为4000，8000，12000，16000和20000时的种群被保存。<br>Region D.设置每个MOP上每个MOEA的运行次数。<br>Region E.设置保存实验设置的文件路径。用户还可以打开现有的配置文件来加载实验设置。所有结果都将保存在文件路径的同一文件夹中。<br>Region F.按顺序或并行执行实验。<br>Region G.显示实验的统计结果。<br>Region H.指定表中显示的数据类型。<br>Region I.将表格保存为Excel或LaTeX格式。<br>Region J.选择表中显示的数据，包括最终总体的任何性能度量值。<br>Region K.控制优化过程，即，开始，暂停，停止。<br>Region L.右键单击一个单元格，以显示种群的Pareto front，种群的pareto集合，或度量值的收敛轮廓。</p><p>用户打开实验模块后，首先选择在区域A中执行的MOEAs和MOPs，并在区域B中设置它们的参数，在区域C中设置保存的种群数量，在区域D中设置运行的数量。然后按下F区域的两个按钮中的一个开始实验。统计结果将显示在G区域的表格中，用户可以使用K区域的按钮来控制优化过程。实验结束后，按下I区按钮，可以将表中数据以Excel或LaTeX格式保存。<br>或者，用户可以通过按区域K中的按钮加载现有配置，然后开始实验。如果在E区域给定的文件夹中已经存在任何结果文件，则将加载结果文件，而不是执行算法。</p><h1 id="扩展platEMO"><a href="#扩展platEMO" class="headerlink" title="扩展platEMO"></a>扩展platEMO</h1><h2 id="platEMO的结构"><a href="#platEMO的结构" class="headerlink" title="platEMO的结构"></a>platEMO的结构</h2><p><img src="//tracywoo.cn/2020/07/10/platEMO-多目标优化工具箱/arch.png" alt="结构"></p><p>进行platEMO平台的实现的三个类是：GLOBAL, INDIVIDUAL和PROBLEMS。其中每个算法都是一个函数，ALGORITHM不是一个类，而是所有算法函数的统一接口。</p><p>GLOBAL表示当前运行的配置，其源代码和详细注释可在Public\GLOBAL.m中找到。在每次执行,维护一个GLOBAL对象来存储所有的参数设置和结果,包括执行的函数处理MOEA和MOP,种群规模,目标的数量,数量的决策变量,函数的最大数量评估,评估个体的数量,等等。GLOBAL还提供了一些MOEAs可以调用的方法，例如，GLOBAL.Initialization()可以生成一个随机的种群初始化，并且GLOBAL.NotTermination()可以检查是否应该终止算法。<br>INDIVIDUAL代表一个个体，源代码及其详细注释可以在Public\INDIVIDUAL.m中找到。一个INDIVIDUAL目标储存决策变量dec、目标值obj、约束违反con和一个个体的附加属性值。在调用构造函数时分配dec和add的值，然后自动计算obj和con的值。上面的每一个性质都是一个行向量，p.decs，P.objs, P.cons或P.adds表示一个决策变量的矩阵，分别为目标值、违反约束或单个INDIVIDUAL对象P数组的附加属性值，其中矩阵的每一行表示一个个体，每一列表示值的一维。当实例化的INDIVIDUAL对象数量超过函数GLOBAL.evaluation计算的最大数量时，算法将被迫终止。<br>PROBLEM是所有问题类的超类。它包含几个方法，即，PROBLEM.Init()用于生成初始种群，PROBLEM.CalDec()用于修复不可行的决策变量，PROBLEM.CalObj()用于计算目标值，PROBLEM.CalCon()用于计算约束违反情况，以及PROBLEM.PF()用于在真正的帕累托前沿采样参考点。每个问题都应该作为PROBLEM的子类来编写，并重载上面的方法。</p><h2 id="增加算法"><a href="#增加算法" class="headerlink" title="增加算法"></a>增加算法</h2><p>MOEA函数由PlatEMO中的一个.m文件表示，该文件应放在“算法”文件夹中。例如，NSGAII.m的源代码是</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">NSGAII</span><span class="params">(Global)</span> </span></span><br><span class="line">  Population = Global.Initialization(); </span><br><span class="line">  [~,FrontNo,CrowdDis] = EnvironmentalSelection(Population,Global.N); </span><br><span class="line">  <span class="keyword">while</span> Global.NotTermination(Population) </span><br><span class="line">      MatingPool = TournamentSelection(<span class="number">2</span>,Global.N,FrontNo, -CrowdDis); </span><br><span class="line">      Offspring = GA(Population(MatingPool)); </span><br><span class="line">      [Population,FrontNo,CrowdDis] = EnvironmentalSelection([Population,Offspring], Global.N); </span><br><span class="line">  <span class="keyword">end</span> </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>首先，用全局变量进行初始化，生成一个初始种群。</p><p>一个多目标进化算法至少有三步：通过Global.Initialization()全局初始化，通过 Global.NotTermination()检查优化是否结束，通过算子（例如GA()）得到子代，EnvironmentalSelection() 是NSGAⅡ特有的算法，  NDSort() 和<br>TournamentSelection()是存储在Public中的实例函数。</p><p>对于基于分解的MOEAs，需要预先生成一组参考点。例如，在MOEAD.m中，使用以下命令生成参考点：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[W,Global.N] = UniformPoint(Global.N,Global.M);</span><br></pre></td></tr></table></figure><p>其中UniformPoint（）是Public文件夹中的一个实例函数，用于在单元超平面上生成具有Global.M目标的Global.N均匀分布点。W是参考点集，总体规模全局。N重置为W中参考点的数量。</p><p>MOEA函数（以及MOP函数）头部应该以指定的形式编写，以便GUI能够识别。例如，在gfmmea.m中，函数头中的注释是</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">GFMMOEA</span><span class="params">(Global)</span> </span></span><br><span class="line"><span class="comment">% &lt;algorithm&gt; &lt;G&gt; </span></span><br><span class="line"><span class="comment">% Generic front modeling based MOEA </span></span><br><span class="line"><span class="comment">% theta --- 0.2 --- Penalty parameter </span></span><br><span class="line"><span class="comment">% fPFE  --- 0.1 --- Frequency of generic front modeling</span></span><br></pre></td></tr></table></figure><p>第2行给出了函数的两个标签，第一个标签&lt;algorithm&gt;表示这是一个MOEA函数（&lt;problem&gt; 和 &lt;metric&gt; 分别表示多目标优化问题和指标）。第二个标签&lt;G&gt;可以是任意字符串。第3行给出了MOEA的全名。第4-5行是GFM-MOEA的参数，其中参数的名称位于每一行的第一列。默认值位于第二列，简介位于第三列；这些列用“-”分隔。然后，GFM-MOEA通过以下命令接收用户的参数设置：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[theta,fPFE] = Global.ParameterSet(<span class="number">0.2</span>,<span class="number">0.1</span>);</span><br></pre></td></tr></table></figure><p>可以在Public\GLOBAL.m中找到Global.ParameterSet()的详细介绍。对于代理辅助的MOEA，可以使用以下命令根据父母的决策变量生成新的决策变量，没有INDIVIDUAL对象会被实例化，并且被评估的个体的数量GLOBAL.evaluated不会增加。</p><p><code>OffDec = GA(Population(MatingPool).decs);</code></p><p>对于函数GA（），如果输入是INDIVIDUAL 对象的数组，则输出也是INDIVIDUAL 对象的数组。如果输入是决策变量的矩阵，那么输出也是决策变量的矩阵。</p><h2 id="增加问题"><a href="#增加问题" class="headerlink" title="增加问题"></a>增加问题</h2><p>MOP函数由PlatEMO中的一个.m文件表示，该文件应该放在文件夹Problems中。<br>例如，DTLZ2的源代码.m是</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="keyword">classdef</span> DTLZ2 &lt; PROBLEM</span><br><span class="line"><span class="number">2.</span>     <span class="keyword">methods</span></span><br><span class="line"><span class="number">3.</span>         <span class="function"><span class="keyword">function</span> <span class="title">obj</span> = <span class="title">DTLZ2</span><span class="params">()</span></span></span><br><span class="line"><span class="number">4.</span>             <span class="keyword">if</span> <span class="built_in">isempty</span>(obj.Global.M)</span><br><span class="line"><span class="number">5.</span>                 obj.Global.M = <span class="number">3</span>;</span><br><span class="line"><span class="number">6.</span>             <span class="keyword">end</span></span><br><span class="line"><span class="number">7.</span>             <span class="keyword">if</span> <span class="built_in">isempty</span>(obj.Global.D)</span><br><span class="line"><span class="number">8.</span>                 obj.Global.D = obj.Global.M + <span class="number">9</span>;</span><br><span class="line"><span class="number">9.</span>             <span class="keyword">end</span></span><br><span class="line"><span class="number">10.</span>            obj.Global.lower = <span class="built_in">zeros</span>(<span class="number">1</span>,obj.Global.D);</span><br><span class="line"><span class="number">11.</span>            obj.Global.upper = <span class="built_in">ones</span>(<span class="number">1</span>,obj.Global.D);</span><br><span class="line"><span class="number">12.</span>            obj.Global.encoding = <span class="string">'real'</span>;</span><br><span class="line"><span class="number">13.</span>         <span class="keyword">end</span></span><br><span class="line"><span class="number">14.</span>         <span class="function"><span class="keyword">function</span> <span class="title">PopObj</span> = <span class="title">CalObj</span><span class="params">(obj,PopDec)</span></span></span><br><span class="line"><span class="number">15.</span>             M = obj.Global.M;</span><br><span class="line"><span class="number">16.</span>             g = sum((PopDec(:,M:<span class="keyword">end</span>)<span class="number">-0.5</span>).^<span class="number">2</span>,<span class="number">2</span>);</span><br><span class="line"><span class="number">17.</span>             PopObj = <span class="built_in">repmat</span>(<span class="number">1</span>+g,<span class="number">1</span>,M).*<span class="built_in">fliplr</span>(cumprod([<span class="built_in">ones</span> (<span class="built_in">size</span>(g,<span class="number">1</span>),<span class="number">1</span>),<span class="built_in">cos</span>(PopDec(:,<span class="number">1</span>:M1)*<span class="built_in">pi</span>/<span class="number">2</span>)],<span class="number">2</span>)).[<span class="built_in">ones</span>(<span class="built_in">size</span>(g,<span class="number">1</span>),<span class="number">1</span>),<span class="built_in">sin</span>(PopDec(:,M<span class="number">-1</span>:<span class="number">-1</span>:<span class="number">1</span>)*<span class="built_in">pi</span>/<span class="number">2</span>)];</span><br><span class="line"><span class="number">18.</span>         <span class="keyword">end</span></span><br><span class="line"><span class="number">19.</span>         <span class="function"><span class="keyword">function</span> <span class="title">P</span> = <span class="title">PF</span><span class="params">(obj,N)</span></span></span><br><span class="line"><span class="number">20.</span>             P = UniformPoint(N,obj.Global.M);</span><br><span class="line"><span class="number">21.</span>             P = P./<span class="built_in">repmat</span>(<span class="built_in">sqrt</span>(sum(P.^<span class="number">2</span>,<span class="number">2</span>)),<span class="number">1</span>,obj.Global.M);</span><br><span class="line"><span class="number">22.</span>         <span class="keyword">end</span></span><br><span class="line"><span class="number">23.</span>     <span class="keyword">end</span></span><br><span class="line"><span class="number">24.</span> <span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>首先，DTLZ2是问题的子类，每个操作都是一个重载方法。在DTLZ2.DTLZ2()构造函数中设置目标个数的值GLOBAL.M，决策变量的数量GLOBAL.D，决策变量的下界GLOBAL.lower，决策变量的上界GLOBAL.upper，和编码GLOBAL.encoding。在DTLZ2.CalObj()中，计算种群的目标值。在DTLZ2.PF()中，真实的帕累托前沿选取一组参考点。</p><p>值得注意的是，决策变量在三种情况下可能是不可行的。首先，对于连续MOPs，它可能大于全局上界（GLOBAL.upper）或小于下界（GLOBAL.lower）。在这种情况下，它将被INDIVIDUAL的类设置为边界值，而MOP类不需要处理这种情况。其次，它可能没有满足约束(一个正的约束违背表明这个约束没有被满足)，在这种情况下，约束违背应该通过重载方法PROBLEM.CalCon()来计算，例如在C1_DTLZ1.m中：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="function"><span class="keyword">function</span> <span class="title">PopCon</span> = <span class="title">CalCon</span><span class="params">(obj,PopDec)</span></span></span><br><span class="line"><span class="number">2.</span>     PopObj = obj.CalObj(PopDec);</span><br><span class="line"><span class="number">3.</span>     PopCon = PopObj(:,<span class="keyword">end</span>)/<span class="number">0.6</span>+sum(PopObj(:,<span class="number">1</span>:<span class="keyword">end</span><span class="number">-1</span>)/<span class="number">0.5</span>,<span class="number">2</span>)<span class="number">-1</span>;</span><br><span class="line"><span class="number">4.</span> <span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>第三，对于组合MOPs，它可能是一个非法字符，在这种情况下，它应该通过重载方法PROBLEM.CalDec()来修复，例如在MOKP.m中：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="function"><span class="keyword">function</span> <span class="title">PopDec</span> = <span class="title">CalDec</span><span class="params">(obj,PopDec)</span></span></span><br><span class="line"><span class="number">2.</span>     C = sum(obj.W,<span class="number">2</span>)/<span class="number">2</span>;</span><br><span class="line"><span class="number">3.</span>     [~,rank] = <span class="built_in">sort</span>(<span class="built_in">max</span>(obj.P./obj.W));</span><br><span class="line"><span class="number">4.</span>     <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">size</span>(PopDec,<span class="number">1</span>)</span><br><span class="line"><span class="number">5.</span>         <span class="keyword">while</span> any(obj.W*PopDec(<span class="built_in">i</span>,:)’&gt;C)</span><br><span class="line"><span class="number">6.</span>             k = <span class="built_in">find</span>(PopDec(<span class="built_in">i</span>,rank),<span class="number">1</span>);</span><br><span class="line"><span class="number">7.</span>             PopDec(<span class="built_in">i</span>,rank(k)) = <span class="number">0</span>;</span><br><span class="line"><span class="number">8.</span>         <span class="keyword">end</span></span><br><span class="line"><span class="number">9.</span>     <span class="keyword">end</span></span><br><span class="line"><span class="number">10.</span> <span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>与MOEA函数类似，MOP类可以接收用户的参数设置，例如在WFG1.m中，以下命令用于接收参数设置：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> obj.K = obj.Global.ParameterSet(obj.Global.M<span class="number">-1</span>);</span><br></pre></td></tr></table></figure><h2 id="增加性能度量"><a href="#增加性能度量" class="headerlink" title="增加性能度量"></a>增加性能度量</h2><p>性能度量函数由PlatEMO中的.m文件表示，该文件应该放在Metrics文件夹中。例如，IGD.m的源代码是</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="function"><span class="keyword">function</span> <span class="title">score</span> = <span class="title">IGD</span>(<span class="params">PopObj,PF</span>)</span></span><br><span class="line">2. % &lt;metric&gt; &lt;min&gt;</span><br><span class="line"><span class="number">3.</span>     score = mean(min(pdist2(PF,PopObj),[],<span class="number">2</span>);</span><br><span class="line"><span class="number">4.</span> end</span><br></pre></td></tr></table></figure><p>首先，性能度量函数有两个输入参数和一个输出参数，其中PopObj表示种群目标值矩阵，PF表示在真实Pareto前沿采样的一组参考点，并且score表示性能度量值。注意，第2行中的注释是GUI标识所必需的，其中第一个标签表示这是一个性能度量函数，第二个标签表示度量值越小性能越好。相反，如果第二个标签是，则表示度量值越大，性能越好。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;platEMO是Ye Tian等学者写的一款基于MATLAB的多目标优化工具。&lt;/p&gt;
&lt;p&gt;这款工具主要具有以下的几个特点：&lt;br&gt;1.完
      
    
    </summary>
    
    
    
      <category term="MOEA" scheme="http://tracywoo.cn/tags/MOEA/"/>
    
  </entry>
  
  <entry>
    <title>VAE_变分自编码器</title>
    <link href="http://tracywoo.cn/2020/07/10/VAE-%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    <id>http://tracywoo.cn/2020/07/10/VAE-%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</id>
    <published>2020-07-10T05:51:25.000Z</published>
    <updated>2020-07-11T02:37:27.755Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>VAE(Variational Auto-Encoder，VAE)是一种生成网络。</p><p>假如我们有一个带有解卷积层的网络，我们设置输入为值全为1的向量，输出为一张图像。然后，我们可以训练这个网络去减小重构图像和原始图像的平均平方误差。那么训练完后，这个图像的信息就被保留在了网络的参数中。</p><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/3.png" alt="带有解卷积层的网络"></p><p>这次我们用one-hot向量而不是全1向量。我们用[1, 0, 0, 0]代表猫，用[0, 1, 0, 0]代表狗。虽然这要没什么问题，但是我们最多只能储存4张图片。当然，我们也可以增加向量的长度和网络的参数，那么我们可以获得更多的图片。</p><p>但是，这样的向量很稀疏。为了解决这个问题，我们想使用实数值向量而不是0，1向量。我们可认为这种实数值向量是原图片的一种编码，这也就引出了编码/解码的概念。举个例子，[3.3, 4.5, 2.1, 9.8]代表猫，[3.4, 2.1, 6.7, 4.2] 代表狗。这个已知的初始向量可以作为我们的潜在变量。</p><p>使用随机初始化来代表图片并不是一个好的手段，我们希望计算机能够帮我们实现自动编码。在Auto-Encoder模型中，我们加入了一个编码器，它能够帮助我们把图片编码程向量，然后解码器能够把这些向量恢复成图片。</p><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/4.png" alt="带有编码器和解卷积层的网络"></p><p>此时获得的网络是标准自编码器，能够将图片的编码向量进行存储，从而进行重构。</p><p>若对于编码器添加约束，强迫它产生服从单位高斯分布的潜在变量。这就是VAE不同于标准自编码器的地方。现在，产生新的图片也变得容易：我们只要从单位高斯分布中进行采样，然后把它传给解码器就可以了。</p><p>对于我们的损失函数，我们可以把这两方面进行加和。一方面，是图片的重构误差，我们可以用平均平方误差来度量，另一方面。我们可以用KL散度来度量我们潜在变量的分布和单位高斯分布的差异。</p><blockquote><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/8.png" alt="loss"></p><p>KL散度可以求两个分布间的距离，KL越小，分布距离越近，即越相似。</p></blockquote><blockquote><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/5.png" alt="loss"></p></blockquote><p>VAE的编码器会产生两个向量:一个是均值向量，一个是标准差向量。</p><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/6.png" alt="ms"></p><p>我们可以这样来计算KL散度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># z_mean and z_stddev are two vectors generated by encoder network</span><br></pre></td></tr></table></figure><blockquote><p>latent_loss = 0.5 * tf.reduce_sum(tf.square(z_mean) + tf.square(z_stddev) - tf.log(tf.square(z_stddev)) - 1,1)</p></blockquote><p>当我们计算解码器的loss时，我们就可以从标准差向量中采样，然后加到我们的均值向量上，就得到了编码去需要的潜在变量。</p><blockquote><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/7.png" alt="la"></p></blockquote><p>VAE除了能让我们能够自己产生随机的潜在变量，这种约束也能提高网络的产生图片的能力。</p><p>为了更加形象，我们可以认为潜在变量是一种数据的转换。</p><p>使用VAE好处就是可以通过编码解码的步骤，直接比较重建图片和原始图片的差异，但是GAN做不到。</p><p> 另外，VAE的一个劣势就是没有使用对抗网络，所以会更趋向于产生模糊的图片。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/8.jpeg" alt="ex"></p><p>p(X|Z) 就描述了一个由 Z 来生成 X 的模型，而我们假设 Z 服从标准正态分布，也就是 p(Z)=N(0,I)。<strong>如果这个理想能实现，那么我们就可以先从标准正态分布中采样一个 Z，然后根据 Z 来算一个 X，也是一个很棒的生成模型</strong>。</p><p>接下来就是结合自编码器来实现重构，保证有效信息没有丢失，再加上一系列的推导，最后把模型实现。框架的示意图如下：</p><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/9.jpeg" alt="ms2"></p><p>其实，<strong>在整个 VAE 模型中，我们并没有去使用 p(Z)（先验分布）是正态分布的假设，我们用的是假设 p(Z|X)（后验分布）是正态分布</strong>。</p><p>具体来说，给定一个真实样本 Xk，我们假设存在<strong>一个专属于 Xk 的分布 p(Z|Xk)</strong>（学名叫后验分布），并进一步假设这个分布是（独立的、多元的）正态分布。</p><p>为什么要强调“专属”呢？因为我们后面要训练一个生成器 X=g(Z)，希望能够把从分布 p(Z|Xk) 采样出来的一个 Zk 还原为 Xk。</p><p>如果假设 p(Z) 是正态分布，然后从 p(Z) 中采样一个 Z，那么我们怎么知道这个 Z 对应于哪个真实的 X 呢？<strong>现在 p(Z|Xk) 专属于 Xk，我们有理由说从这个分布采样出来的 Z 应该要还原到 Xk 中去</strong>。</p><p>论文 Auto-Encoding Variational Bayes的应用部分，也特别强调了这一点：</p><blockquote><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/10.png" alt="for"></p></blockquote><p><strong>那我怎么找出专属于 Xk 的正态分布 p(Z|Xk) 的均值和方差呢？</strong>好像并没有什么直接的思路。</p><p>那好吧，<strong>我就用神经网络来拟合出来</strong>。</p><p>于是我们构建两个神经网络 μk=f1(Xk)，logσ^2=f2(Xk) 来算它们了。我们选择拟合 logσ^2 而不是直接拟合 σ^2，是因为 σ^2 总是非负的，需要加激活函数处理，而拟合 logσ^2 不需要加激活函数，因为它可正可负。</p><p>到这里，我能知道专属于 Xk 的均值和方差了，也就知道它的正态分布长什么样了，然后从这个专属分布中采样一个 Zk 出来，然后经过一个生成器得到 X̂k=g(Zk)。</p><p>现在我们可以放心地最小化 D(X̂k,Xk)^2，因为 Zk 是从专属 Xk 的分布中采样出来的，这个生成器应该要把开始的 Xk 还原回来。<strong>于是可以画出 VAE 的示意图：</strong></p><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/11.jpeg" alt="for2"></p><p>事实上，VAE 是为每个样本构造专属的正态分布，然后采样来重构。</p><p>让我们来思考一下，根据上图的训练过程，最终会得到什么结果。</p><p>首先，我们希望重构 X，也就是最小化 D(X̂k,Xk)^2，但是这个重构过程受到噪声的影响，因为 Zk 是通过重新采样过的，不是直接由 encoder 算出来的。</p><p>显然噪声会增加重构的难度，不过好在这个噪声强度（也就是方差）通过一个神经网络算出来的，所以最终模型为了重构得更好，肯定会想尽办法让方差为0。</p><p>而方差为 0 的话，也就没有随机性了，所以不管怎么采样其实都只是得到确定的结果（也就是均值），只拟合一个当然比拟合多个要容易，而均值是通过另外一个神经网络算出来的。</p><p>说白了，<strong>模型会慢慢退化成普通的 AutoEncoder，噪声不再起作用</strong>。</p><p>这样不就白费力气了吗？说好的生成模型呢？</p><p>别急别急，<strong>VAE 还让所有的 p(Z|X) 都向标准正态分布看齐</strong>，这样就防止了噪声为零，同时保证了模型具有生成能力。</p><p>如果所有的 p(Z|X) 都很接近标准正态分布 N(0,I)，那么根据定义：</p><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/13.png" alt="for3"></p><p>这样我们就能达到我们的先验假设：p(Z) 是标准正态分布。然后我们就可以放心地从 N(0,I) 中采样来生成图像了。</p><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/14.jpeg" alt="for2"></p><p>在VAE中使用了重参数的技巧(Reparameterization Trick)</p><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/15.png" alt="for2"></p><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/16.png" alt="for2"></p><h1 id="本质"><a href="#本质" class="headerlink" title="本质"></a>本质</h1><p>VAE 虽然也称是 AE（AutoEncoder）的一种，但它的做法（或者说它对网络的诠释）是别具一格的。</p><p>在 VAE 中，它的 Encoder 有两个，一个用来计算均值，一个用来计算方差。</p><p><strong>它本质上就是在我们常规的自编码器的基础上，对 encoder 的结果（在VAE中对应着计算均值的网络）加上了“高斯噪声”，使得结果 decoder 能够对噪声有鲁棒性；而那个额外的 KL loss（目的是让均值为 0，方差为 1），事实上就是相当于对 encoder 的一个正则项，希望 encoder 出来的东西均有零均值。</strong></p><p>那另外一个 encoder（对应着计算方差的网络）的作用呢？它是用来<strong>动态调节噪声的强度</strong>的。</p><p>直觉上来想，<strong>当 decoder 还没有训练好时（重构误差远大于 KL loss），就会适当降低噪声（KL loss 增加），使得拟合起来容易一些（重构误差开始下降）</strong>。</p><p>反之，<strong>如果 decoder 训练得还不错时（重构误差小于 KL loss），这时候噪声就会增加（KL loss 减少），使得拟合更加困难了（重构误差又开始增加），这时候 decoder 就要想办法提高它的生成能力了</strong>。</p><p><img src="//tracywoo.cn/2020/07/10/VAE-变分自编码器/17.jpeg" alt="for2"></p><p><strong>重构的过程是希望没噪声的，而 KL loss 则希望有高斯噪声的，两者是对立的。所以，VAE 跟 GAN 一样，内部其实是包含了一个对抗的过程，只不过它们两者是混合起来，共同进化的</strong>。</p><p><strong>在 VAE 中，重构跟噪声是相互对抗的，重构误差跟噪声强度是两个相互对抗的指标，而在改变噪声强度时原则上需要有保持均值不变的能力，不然我们很难确定重构误差增大了，究竟是均值变化了（encoder的锅）还是方差变大了（噪声的锅）</strong>。</p><p>参考资料：</p><p><a href="https://www.sohu.com/a/226209674_500659" target="_blank" rel="noopener">变分自编码器VAE：原来是这么一回事 | 附开源代码</a></p><p><a href="https://www.jeremyjordan.me/variational-autoencoders/" target="_blank" rel="noopener">VAE</a></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>附上在tensorflow上实现VAE的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># x_hat, n_hidden, dim_z, keep_prob</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_MLP_encoder</span><span class="params">(x, n_hidden, n_output, keep_prob)</span>:</span></span><br><span class="line">    <span class="comment"># Gaussian MLP as encoder 构造编码器</span></span><br><span class="line">    <span class="comment"># input: x(输入数据), n_hidden(隐藏层个数), n_output(输出层个数), keep_prob()</span></span><br><span class="line">    <span class="comment"># output: mean(均值) ,stddev(方差)</span></span><br><span class="line">    print(<span class="string">"encode:\n"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"gaussian_MLP_encoder"</span>):</span><br><span class="line">        <span class="comment"># 初始化 w b</span></span><br><span class="line">        <span class="comment"># scale=1.0,mode="fan_in",distribution="normal",seed=None，dtype=dtypes.float32</span></span><br><span class="line">        w_init = tf.contrib.layers.variance_scaling_initializer()</span><br><span class="line">        b_init = tf.constant_initializer(<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1st hidden layer</span></span><br><span class="line">        w0 = tf.get_variable(<span class="string">'w0'</span>, [x.get_shape()[<span class="number">1</span>], n_hidden], initializer=w_init)</span><br><span class="line">        b0 = tf.get_variable(<span class="string">'b0'</span>, [n_hidden], initializer=b_init)</span><br><span class="line">        h0 = tf.matmul(x, w0) + b0</span><br><span class="line">        <span class="comment"># elu为激活函数 &lt;0 exp(x)-1  &gt;0 =x</span></span><br><span class="line">        h0 = tf.nn.elu(h0)</span><br><span class="line">        h0 = tf.nn.dropout(h0, keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2nd hidden layer 隐含层函数</span></span><br><span class="line">        w1 = tf.get_variable(<span class="string">'w1'</span>, [h0.get_shape()[<span class="number">1</span>], n_hidden], initializer=w_init)</span><br><span class="line">        b1 = tf.get_variable(<span class="string">'b1'</span>, [n_hidden], initializer=b_init)</span><br><span class="line">        h1 = tf.matmul(h0, w1) + b1</span><br><span class="line">        h1 = tf.nn.tanh(h1)</span><br><span class="line">        h1 = tf.nn.dropout(h1, keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output layer 输出层</span></span><br><span class="line">        wo = tf.get_variable(<span class="string">'wo'</span>, [h1.get_shape()[<span class="number">1</span>], n_output * <span class="number">2</span>], initializer=w_init)</span><br><span class="line">        bo = tf.get_variable(<span class="string">'bo'</span>, [n_output * <span class="number">2</span>], initializer=b_init)</span><br><span class="line">        gaussian_params = tf.matmul(h1, wo) + bo</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The mean parameter is unconstrained</span></span><br><span class="line">        mean = gaussian_params[:, :n_output]</span><br><span class="line">        <span class="comment"># The standard deviation must be positive. Parametrize with a softplus and</span></span><br><span class="line">        <span class="comment"># add a small epsilon for numerical stability</span></span><br><span class="line">        stddev = <span class="number">1e-6</span> + tf.nn.softplus(gaussian_params[:, n_output:])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mean, stddev</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bernoulli_MLP_decoder</span><span class="params">(z, n_hidden, n_output, keep_prob, reuse=False)</span>:</span></span><br><span class="line">    <span class="comment"># Bernoulli MLP as decoder</span></span><br><span class="line">    <span class="comment"># input: z(隐变量), n_hidden(隐藏层个数),n_output(输出个数), keep_prob(), reuse()</span></span><br><span class="line">    print(<span class="string">"decode"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"bernoulli_MLP_decoder"</span>, reuse=reuse):</span><br><span class="line">        <span class="comment"># initializers</span></span><br><span class="line">        w_init = tf.contrib.layers.variance_scaling_initializer()</span><br><span class="line">        b_init = tf.constant_initializer(<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1st hidden layer</span></span><br><span class="line">        w0 = tf.get_variable(<span class="string">'w0'</span>, [z.get_shape()[<span class="number">1</span>], n_hidden], initializer=w_init)</span><br><span class="line">        b0 = tf.get_variable(<span class="string">'b0'</span>, [n_hidden], initializer=b_init)</span><br><span class="line">        h0 = tf.matmul(z, w0) + b0</span><br><span class="line">        h0 = tf.nn.tanh(h0)</span><br><span class="line">        h0 = tf.nn.dropout(h0, keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2nd hidden layer</span></span><br><span class="line">        w1 = tf.get_variable(<span class="string">'w1'</span>, [h0.get_shape()[<span class="number">1</span>], n_hidden], initializer=w_init)</span><br><span class="line">        b1 = tf.get_variable(<span class="string">'b1'</span>, [n_hidden], initializer=b_init)</span><br><span class="line">        h1 = tf.matmul(h0, w1) + b1</span><br><span class="line">        h1 = tf.nn.elu(h1)</span><br><span class="line">        h1 = tf.nn.dropout(h1, keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output layer-mean</span></span><br><span class="line">        wo = tf.get_variable(<span class="string">'wo'</span>, [h1.get_shape()[<span class="number">1</span>], n_output], initializer=w_init)</span><br><span class="line">        bo = tf.get_variable(<span class="string">'bo'</span>, [n_output], initializer=b_init)</span><br><span class="line">        y = tf.sigmoid(tf.matmul(h1, wo) + bo)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoencoder</span><span class="params">(x_hat, x, dim_img, dim_z, n_hidden, keep_prob)</span>:</span></span><br><span class="line">    <span class="comment"># input: x_hat(input), x(), dim_img(284**2), dim_z(-&gt; n_output), n_hidden(500), keep_prob(0.9 dropout)</span></span><br><span class="line">    <span class="comment"># encodingGateway</span></span><br><span class="line">    <span class="comment"># 编码得到方差和均值</span></span><br><span class="line">    mu, sigma = gaussian_MLP_encoder(x_hat, n_hidden, dim_z, keep_prob)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到</span></span><br><span class="line">    <span class="comment"># sampling by re-parameterization technique</span></span><br><span class="line">    z = mu + sigma * tf.random_normal(tf.shape(mu), <span class="number">0</span>, <span class="number">1</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># decoding</span></span><br><span class="line">    y = bernoulli_MLP_decoder(z, n_hidden, dim_img, keep_prob)</span><br><span class="line">    <span class="comment"># 对数据的限制 y&lt;1e-8 则y=1e-8 y&gt;1-1e-8 则y=1-1e-8 否则y=原值</span></span><br><span class="line">    y = tf.clip_by_value(y, <span class="number">1e-8</span>, <span class="number">1</span> - <span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss</span></span><br><span class="line">    marginal_likelihood = tf.reduce_sum(x * tf.log(y) + (<span class="number">1</span> - x) * tf.log(<span class="number">1</span> - y), <span class="number">1</span>)</span><br><span class="line">    KL_divergence = <span class="number">0.5</span> * tf.reduce_sum(tf.square(mu) + tf.square(sigma) - tf.log(<span class="number">1e-8</span> + tf.square(sigma)) - <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算张量沿着指定的数轴上的平均值 用于降维或者计算tensor的平均值</span></span><br><span class="line">    marginal_likelihood = tf.reduce_mean(marginal_likelihood)</span><br><span class="line">    KL_divergence = tf.reduce_mean(KL_divergence)</span><br><span class="line"></span><br><span class="line">    ELBO = marginal_likelihood - KL_divergence</span><br><span class="line"></span><br><span class="line">    loss = -ELBO</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y, z, loss, -marginal_likelihood, KL_divergence</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span><span class="params">(z, dim_img, n_hidden)</span>:</span></span><br><span class="line">    y = bernoulli_MLP_decoder(z, n_hidden, dim_img, <span class="number">1.0</span>, reuse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;VAE(Variational Auto-Encoder，VAE)是一种生成网络。&lt;/p&gt;
&lt;p&gt;假如我们有一个带有解卷积层的网络，我们设置
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://tracywoo.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>博客的重新配置</title>
    <link href="http://tracywoo.cn/2020/07/10/%E5%8D%9A%E5%AE%A2%E7%9A%84%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/"/>
    <id>http://tracywoo.cn/2020/07/10/%E5%8D%9A%E5%AE%A2%E7%9A%84%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/</id>
    <published>2020-07-10T00:38:08.000Z</published>
    <updated>2020-07-12T00:06:31.450Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>换了电脑以后一直没有鼓起勇气重新安装各种软件（怕麻烦本人哈哈哈），阿里云的email提醒我域名该续费了，续完费发现自己又把之前的操作什么的忘得一干二净=-=，于是又开始配置。痛定思痛，以后要好好写博客记录自己的所学所想哈哈哈哈，奥里给，冲鸭！</p><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="如何将之前的博客部署到新的电脑上？"><a href="#如何将之前的博客部署到新的电脑上？" class="headerlink" title="如何将之前的博客部署到新的电脑上？"></a>如何将之前的博客部署到新的电脑上？</h2><p>我是把之前的与博客的相关文件直接复制到新电脑上，重新安装了Git和Node.js，但是此时进入blog文件夹进行hexo部署时一直报错：</p><p><code>Please make sure you have the correct access rights and the repository exist</code></p><p><code>The authenticity of host &#39;github.com (xx.xxx.xxx.xxx)&#39; can&#39;t be established.</code></p><p><code>unable to auto-detect email address (got &#39;11647@LAPTOP-2902H8OD.(none)&#39;)</code></p><p>查了各方的资料，一直报错，但是大致知道是因为SSH密钥和电脑的连接出现了问题。最后问题的解决大致流程如下：</p><p>进入系统的SSH文件夹，使用 <code>git bash</code> 设置用户名和邮箱（我不知道是不是和要github中的一致，但是我一开始设置不一致的时候设置了密码，我也不清楚是因为设置了密码访问不成功还是因为不一致导致的）</p><p><em>git config –global user.name “用户名”</em></p><p><em>git config –global user.email “你的邮箱”</em></p><p>设置完毕之后，执行命令<em>ssh-keygen -t rsa -C “github注册邮箱”</em>，执行完成后，会生成一个.ssh文件夹，里面的id_rsa.pub文件内容就是秘钥，那么我们就进入ssh文件夹打开该文件后复制它的内容。进入GitHub个人账户的settings-&gt;SSH and GPG keys，将SSH密钥添加到自己的仓库设置中。</p><p>接着在hexo deploy时如果出现<br><code>The authenticity of host &#39;github.com (192.30.253.112)&#39; can&#39;t be established.</code><br><code>RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.</code><br><code>Are you sure you want to continue connecting (yes/no)?</code></p><p>输入yes回车即可。</p><h2 id="文章撰写的一些基础知识"><a href="#文章撰写的一些基础知识" class="headerlink" title="文章撰写的一些基础知识"></a>文章撰写的一些基础知识</h2><p>新建一篇文章：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new &quot;title&quot;</span><br></pre></td></tr></table></figure><p>部署发布：</p><p>在站点文件夹中打开 git bash，输入如下命令部署和发布文章</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g -d</span><br></pre></td></tr></table></figure><p><strong>建议</strong>：在使用 <code>hexo g</code> 部署之后，可以先使用 <code>hexo s</code> 运行本地站点，然后在浏览器输入地址 <a href="http://lacolhost:4000/" target="_blank" rel="noopener">http://lacolhost:4000/</a> 查看运行结果，检查无误后再使用 <code>hexo d</code> 发布</p><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><p><strong>Markdown 语法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 一级标题</span><br><span class="line">## 二级标题</span><br><span class="line">### 三级标题</span><br><span class="line">#### 四级标题</span><br><span class="line">##### 五级标题</span><br><span class="line">###### 六级标题</span><br></pre></td></tr></table></figure><p><strong>Typora 快捷键：</strong></p><blockquote><p>Ctrl+1：一级标题</p><p>Ctrl+2：二级标题</p><p>Ctrl+3：三级标题</p><p>Ctrl+4：四级标题</p><p>Ctrl+5：五级标题</p><p>Ctrl+6 ：六级标题</p><p>Ctrl+0：段落</p></blockquote><h3 id="粗体、斜体、删除线和下划线"><a href="#粗体、斜体、删除线和下划线" class="headerlink" title="粗体、斜体、删除线和下划线"></a>粗体、斜体、删除线和下划线</h3><p><strong>Markdown 语法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">*斜体*</span><br><span class="line">**粗体**</span><br><span class="line">***加粗斜体***</span><br><span class="line">~~删除线~~</span><br></pre></td></tr></table></figure><p><strong>Typora 快捷键：</strong></p><blockquote><p>Ctrl+I：斜体</p><p>Ctrl+B：粗体</p><p>Ctrl+U：下划线</p><p>Alt+Shift+5：删除线</p></blockquote><h3 id="引用块"><a href="#引用块" class="headerlink" title="引用块"></a>引用块</h3><p><strong>Markdown 语法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; 文字引用</span><br></pre></td></tr></table></figure><p><strong>Typora 快捷键：</strong> </p><blockquote><p>Ctrl+Shift+Q</p></blockquote><h3 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h3><p><strong>Markdown 语法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`行内代码`</span><br></pre></td></tr></table></figure><p><strong>Typora 快捷键：</strong></p><blockquote><p>行内代码：Ctrl+Shift+`</p><p>多行代码：Ctrl+Shift+K</p></blockquote><h3 id="公式块"><a href="#公式块" class="headerlink" title="公式块"></a>公式块</h3><p><strong>Markdown 语法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">数学公式</span><br><span class="line">$$</span><br><span class="line">123</span><br></pre></td></tr></table></figure><p><strong>Typora 快捷键：</strong> </p><blockquote><p>Ctrl+Shift+M</p></blockquote><h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><p><strong>Markdown 语法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">方法一：---</span><br><span class="line"></span><br><span class="line">方法二：+++</span><br><span class="line"></span><br><span class="line">方法三：***</span><br></pre></td></tr></table></figure><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><p><strong>Markdown 语法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. 有序列表项</span><br><span class="line"></span><br><span class="line">* 无序列表项</span><br><span class="line"></span><br><span class="line">+ 无序列表项</span><br><span class="line"></span><br><span class="line">- 无序列表项</span><br></pre></td></tr></table></figure><p><strong>Typora 快捷键：</strong></p><blockquote><p>有序列表项：Ctrl+Shift+[</p><p>无序列表项：Ctrl+Shift+]</p></blockquote><h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><p><strong>Markdown 语法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">表头1|表头2</span><br><span class="line">-|-|-</span><br><span class="line">内容11|内容12</span><br><span class="line">内容21|内容22</span><br></pre></td></tr></table></figure><p><strong>Typora 快捷键：</strong> Ctrl+T</p><h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><p><strong>Markdown语法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">方法一：[链接文字](链接地址 &quot;链接描述&quot;)</span><br><span class="line">例如：[示例链接](https://www.example.com/ &quot;示例链接&quot;)</span><br><span class="line"></span><br><span class="line">方法二：&lt;链接地址&gt;</span><br><span class="line">例如：&lt;https://www.example.com/&gt;</span><br></pre></td></tr></table></figure><p><strong>Typora快捷键：</strong> </p><blockquote><p> Ctrl+K</p></blockquote><h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><p><strong>Markdown语法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">![图片文字](图片地址 &quot;图片描述&quot;)</span><br><span class="line">例如：![示例图片](https://www.example.com/example.PNG &quot;示例图片&quot;)</span><br></pre></td></tr></table></figure><p>Typora快捷键： </p><blockquote><p>Ctrl+Shift+I</p></blockquote><p>说明：在 Hexo中 插入图片时，请按照以下的步骤进行设置</p><p>将 站点配置文件 中的 <em>post_asset_folder</em> 选项的值设置为 <em>true</em></p><p>在站点文件夹中打开 <em>git bash*，输入命令 *npm install hexo-asset-image –save</em> 安装插件</p><p>这样，当使用 hexo new title 创建文章时，将同时在 <em>source/_post</em> 文件夹中生成一个与 <em>title</em> 同名的文件夹，我们只需将图片放进此文件夹中，然后在文章中通过 Markdown 语法进行引用即可</p><p>例如，在资源文件夹（就是那个与 title 同名的文件夹）中添加图片 example.PNG，则可以在对应的文章中使用语句 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![示例图片](title/example.PNG &quot;示例图片&quot;)</span><br></pre></td></tr></table></figure><p> 添加图片</p><h2 id="其他设置"><a href="#其他设置" class="headerlink" title="其他设置"></a>其他设置</h2><h3 id="模板设置"><a href="#模板设置" class="headerlink" title="模板设置"></a>模板设置</h3><p>当我们使用命令 <code>hexo new &quot;title&quot;</code> 创建文章时，Hexo 会根据 <code>/scaffolds/post.md</code> 对新文章进行初始化。</p><p>换言之，<code>/scaffolds/post.md</code> 就是新文章的 <strong>模板</strong>，所以我们可以修改它来适应自己的写作习惯</p><p>一个简单的示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags: </span><br><span class="line">categories:</span><br></pre></td></tr></table></figure><h3 id="头部设置"><a href="#头部设置" class="headerlink" title="头部设置"></a>头部设置</h3><p>在每篇利用 Hexo 创建的文章的开头，都会有对文章进行说明的文字，叫做 <strong>文章头部</strong></p><p>文章的头部除了可以设置文章标题、发布日期等基础信息外，还可以为文章添加标签、分类等</p><p>一个简单的示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">title: Title</span><br><span class="line">date: YYYY-MM-DD HH:MM:SS</span><br><span class="line">tags: [tag1, tag2, ...]</span><br><span class="line">categories: category</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：属性和属性值之间必须有一个空格，否则会解析错误。</p><h3 id="首页显示"><a href="#首页显示" class="headerlink" title="首页显示"></a>首页显示</h3><p>在利用 Hexo 框架搭建的博客网站中，首页会显示文章的内容，且默认显示文章的全部内容</p><p>如果当文章太长的时候就会显得十分冗余，所以我们有必要对其进行精简</p><p>这时，我们只需在文章中使用 <code>&lt;!--more--&gt;</code> 标志即可，表示只会显示标志前面的内容</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;换了电脑以后一直没有鼓起勇气重新安装各种软件（怕麻烦本人哈哈哈），阿里云的email提醒我域名该续费了，续完费发现自己又把之前的操作什么的忘
      
    
    </summary>
    
    
      <category term="博客" scheme="http://tracywoo.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="hexo" scheme="http://tracywoo.cn/tags/hexo/"/>
    
      <category term="git" scheme="http://tracywoo.cn/tags/git/"/>
    
      <category term="node" scheme="http://tracywoo.cn/tags/node/"/>
    
      <category term="markdown" scheme="http://tracywoo.cn/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>ROC和AUC</title>
    <link href="http://tracywoo.cn/2020/03/21/ROC%E5%92%8CAUC/"/>
    <id>http://tracywoo.cn/2020/03/21/ROC%E5%92%8CAUC/</id>
    <published>2020-03-21T02:53:55.000Z</published>
    <updated>2021-05-08T11:47:17.982Z</updated>
    
    <content type="html"><![CDATA[<p><strong>AUC</strong></p><p>Area Under Curve，ROC曲线下的面积，可以用来评估模型整体的排序能力，这个面积的数值不会大于1，由于ROC曲线一般都处于y=x上方，所以AUC的值一般在0.5-1.0之间。</p><h2 id="ROC是什么"><a href="#ROC是什么" class="headerlink" title="ROC是什么"></a>ROC是什么</h2><h4 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h4><p>比如LR问题，通过模型可以对于每个样本输出一个概率值，代表每个样本是正样本的概率，ROC是根据这些输出的概率以及样本真实label计算画出的。</p><p><strong>横轴</strong>：负正类特异度(FPR)，划分实例中所有负例占所有负例的比例。</p><p><strong>纵轴</strong>：真正类率灵敏度(TPR)。</p><h5 id="四种类别"><a href="#四种类别" class="headerlink" title="四种类别"></a>四种类别</h5><p><strong>TP</strong>:正确的正样本数目</p><p><strong>TF</strong>:正确的负样本数目</p><p><strong>FP</strong>:错误的正样本数目</p><p><strong>FN</strong>:错误的负样本数目</p><p><img src="https://images0.cnblogs.com/blog2015/712297/201504/081953479158732.jpg" alt="img"></p><p>真正类率(True Postive Rate)<strong>TPR</strong>: <strong>TP/(TP+FN)</strong>,代表分类器预测的正类中实际正实例占所有正实例的比例。Sensitivity</p><p>负正类率(False Postive Rate)<strong>FPR</strong>: <strong>FP/(FP+TN)</strong>，代表分类器预测的正类中实际负实例占所有负实例的比例。1-Specificity</p><p>真负类率(True Negative Rate)<strong>TNR</strong>: <strong>TN/(FP+TN)</strong>,代表分类器预测的负类中实际负实例占所有负实例的比例，TNR=1-FPR。Specificity</p><p>假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR),在平面中得到对应坐标点。随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着真正的负实例，即TPR和FPR会同时增大。阈值最大时，对应坐标点为(0,0),阈值最小时，对应坐标点(1,1)。</p><p>如下面这幅图，(a)图中实线为ROC曲线，线上每个点对应一个阈值。</p><p><img src="https://images0.cnblogs.com/blog2015/712297/201504/081954327748728.jpg" alt="img"></p><p>横轴FPR:1-TNR,1-Specificity，FPR越大，预测正类中实际负类越多。</p><p>纵轴TPR：Sensitivity(正类覆盖率),TPR越大，预测正类中实际正类越多。</p><p>理想目标：TPR=1，FPR=0,即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。</p><p>当FPrate=TPrate时，y=x。含义是对于不论真实类别是1还是0的样本，分类为1的概率是相等的，对于正样本和负样本的没有区分能力。</p><p>在二分类问题中，一般最终的输出是一个概率值，不同的threshold对应的分类结果也是不同的，当阈值从0-&gt;1会形成很多对(FPR,TPR)，将他们画在坐标系中，就形成了ROC曲线。</p><p>阈值的范围欸为[0,1]，阈值从1到0移动时，FP会越来越多。</p><p>ROC曲线不是光滑的，往往呈阶梯型，因为样本的数量是有限的，至少需要一个样本的变化，步进是1/样本数。</p><h2 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h2><p>AUC(Area under Curve)：Roc曲线下的面积，介于0.1和1之间。Auc作为数值可以直观的评价分类器的好坏，值越大越好。</p><p>首先AUC值是一个概率值，当你随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值，AUC值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。</p><p>假设我们设置k从1开始，k只需要从大到小地遍历所有的预测值，就可以得到ROC曲线上的所有点了。</p><p>记正样本个数为m+，负样本个数为m−，取k=1，此时所有样例都被分为反例，因此真正例和假正例的比率均为0，对应原点(0,0)，若前一个标记点是(x,y)，若下一个加入到正例中的样本真实类别是正例，说明我们多了一个TP，回顾真正例率的计算公式，正例样本不变，多了一个正例，则正例率相应提高1/m+，对应标记点为(x,y+1/m+)，若下一个加入到正例中的样本真实类别是反例，则多了一个FP，假反利率相应提高1/m−，则对应标记点为(x+1/m−,y)，然后用线段连接相应的点即可得到ROC曲线。</p><p><img src="//tracywoo.cn/2020/03/21/ROC和AUC/image-20200321104358687.png" alt="image-20200321104358687"></p><p>同时考虑了正例和负例的分类能力，在样本不平衡的情况下，以九可以对于分类器作出合理的评价。</p><p><a href="https://blog.csdn.net/liweibin1994/article/details/79462554" target="_blank" rel="noopener">https://blog.csdn.net/liweibin1994/article/details/79462554</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;AUC&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Area Under Curve，ROC曲线下的面积，可以用来评估模型整体的排序能力，这个面积的数值不会大于1，由于ROC曲线一般都处于y=x上方，所以AUC的值一般在0.5-1.0之间。&lt;/p&gt;
&lt;h2 id=&quot;RO
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://tracywoo.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>基于AUC最大化的同时进行异常值检测和特征选择的PU分类</title>
    <link href="http://tracywoo.cn/2020/03/21/%E5%9F%BA%E4%BA%8EAUC%E6%9C%80%E5%A4%A7%E5%8C%96%E7%9A%84%E5%90%8C%E6%97%B6%E8%BF%9B%E8%A1%8C%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B%E5%92%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84PU%E5%88%86%E7%B1%BB/"/>
    <id>http://tracywoo.cn/2020/03/21/%E5%9F%BA%E4%BA%8EAUC%E6%9C%80%E5%A4%A7%E5%8C%96%E7%9A%84%E5%90%8C%E6%97%B6%E8%BF%9B%E8%A1%8C%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B%E5%92%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84PU%E5%88%86%E7%B1%BB/</id>
    <published>2020-03-21T02:50:05.000Z</published>
    <updated>2021-05-08T11:47:40.137Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>PU学习的应用十分广泛，可以应用于卫生保健、文本分类以及生物信息领域，对于复杂的数据，一个鲁棒的PU学习算法是十分必要的，因此本文提出了公式化的AUC最大化算法，可以在分类的同时检测错误的标签，去除冗余的特征。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>PU学习：给定少数的正样本和很多的负样本，从中学习一个二分类器。我们考虑了以下三种复杂数据：</p><p>​    数据的不均匀，当存在的正样本少于0.1%时，分类器将所有的样本识别为负的准确率高达99.9%。</p><p>​    标记的正样本存在错误的标签。</p><p>​    样本存在冗余或者无关的特征。</p><p>本文提出的算法，同时考虑了以上三个特点的复杂数据。提出了一个鲁棒的学习框架，可以统一AUC（曲线下的面积）最大化-一种用于偏差标签，异常值检测（用于排除错误标签和不良样本）和特征选择（用于排除冗余的特征）。AUC比召回率更可靠，但是由于PU学习中不存在负样本，本文提出了<strong>BAUC</strong>，并证明了<strong>BAUC</strong>的最大化可以代表AUC的最大化。特征选择可以解决过拟合问题。</p><p>本文提出的BAUC-OF模型比已经存在的模型更加鲁棒，不用设置先验值prior，异常检测和特征选择都与AUC最大化公式集成在一起。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h4 id="从PU数据中学习"><a href="#从PU数据中学习" class="headerlink" title="从PU数据中学习"></a>从PU数据中学习</h4><p>解决PU学习的传统方法是简单地将所有未标记的数据视为否定样本，这可能会导致解决方案有偏差。为了减轻这种偏差，提出了几种方法。仅在训练集中使用阳性样本；进一步引入了一些非凸损失函数来减轻这种偏差；提出了凸公式，该凸公式仍然可以消除PU问题的这种偏差。在某些已知先验先验的应用中，可以将PU学习简化为解决成本敏感的分类问题；研究了根据先验类来调整损失函数内部权重的工作。但是，对类先验的不正确估计会增加分类误差。因此，在PU问题中的一些研究工作还研究了估计类先验的不同方法；提出的其他方法还包括基于图的方法；基于引导程序的方法。</p><h4 id="异常值检测"><a href="#异常值检测" class="headerlink" title="异常值检测"></a>异常值检测</h4><p>就我们所知，PU分类中未考虑异常值问题。常规设置中的异常值检测方法可以粗略地分类为基于方差的方法和基于模型的方法。<br><strong>基于方差的方法</strong>通过一组标准来检测离群值，该标准用于测量数据与数据集其余部分之间的差异，这些标准可以来自统计分析、距离度量、密度比。当数据量不太大时，它们通常在低维空间中工作良好。但是很难将它们集成到其他模型中，例如PU学习。<br><strong>基于模型的方法</strong>通过某些模型检测异常值。典型的方法包括基于正则化的主成分回归、正则化的偏最小二乘、基于SVM的算法。<br>这些作品均未与PU学习模型集成。</p><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>特征选择/过拟合特征选择在机器学习中非常重要，特别是对于特征数量远大于数据点数量的应用。虽然特征选择具有广泛的方法，但是在这里，稀疏的学习领域与我们的研究更加相关。通常，有两种类型的稀疏学习方法。<br>一类方法包括<strong>凸松弛公式</strong>，由基于ℓ1范数的方法表示，例如开发的LASSO公式。<br>另一种方法是<strong>非凸公式</strong>，由基于ℓ0范数（或贪婪）的方法表示，例如OMP和基于投影梯度的方法。通常，基于ℓ1的方法是基于ℓ0范数的方法的凸松弛。除此之外也有许多理论研究，以比较基于ℓ0范数的方法与基于ℓ1的方法的性能。特征选择框架也扩展到多类设置。</p><h2 id="可以同时进行异常检测和特征选择的BAUC公式"><a href="#可以同时进行异常检测和特征选择的BAUC公式" class="headerlink" title="可以同时进行异常检测和特征选择的BAUC公式"></a>可以同时进行异常检测和特征选择的BAUC公式</h2><h4 id="BAUC"><a href="#BAUC" class="headerlink" title="BAUC"></a>BAUC</h4><h5 id="AUC公式"><a href="#AUC公式" class="headerlink" title="AUC公式"></a>AUC公式</h5><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321094734021.png" alt="image-20200321094734021"></p><p>其中，f是样本的预测结果，D+是正样本分布，D-是负样本分布。1是一个指示符函数，如果满足条件则返回1，否则返回0.AUC计算的是从正样本和负样本中随机抽取时，正样本比负样本得分高的概率。</p><p>PU实验中不存在负样本，所以引入了BAUC。</p><h5 id="BAUC-1"><a href="#BAUC-1" class="headerlink" title="BAUC"></a>BAUC</h5><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321094747158.png" alt="image-20200321094747158"></p><p>其中D是未标记样本的分布，使用BAUC，我们可以得出以下经验公式，以从正训练集X +和未标记训练集X中学习分类器f：</p><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321094856045.png" alt="image-20200321094856045"><em>（1）</em></p><p>可以由随机变量之和等于各个期望值的总和得以证明：</p><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321094925107.png" alt="image-20200321094925107"></p><p>使用hinge loss或者logistic loss之类的代替函数代替指标函数1可以近似的最大化公式。</p><p>BAUC和AUC之间存在线性依赖关系，可以通过最大化BAUC来达到最大化AUC的目的。</p><p><strong>理论1</strong>：</p><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321095312888.png" alt="image-20200321095312888"></p><p>其中pai是正样本所占的百分比。</p><p>从一个数据集随机选择的阳性样本高于另一个随机选择的阳性样本的概率始终是1/2.</p><h4 id="集成的BAUCv公式"><a href="#集成的BAUCv公式" class="headerlink" title="集成的BAUCv公式"></a>集成的BAUCv公式</h4><p>BAUC的最大化可以转化为最小化问题：</p><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321095527511.png" alt="image-20200321095527511"></p><p>构造了一个向量来进行异常值（误标、特征值损坏）的检测。</p><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321095652637.png" alt="image-20200321095652637"></p><p>此公式背后的主要动机是使用向量来<strong>调整异常值x +的得分</strong>，而不是修改其标签特征值，尽管它实际上具有同等的效果。 （3）中的约束是通过用户定义的参数t限制异常值的最大数量，并且最佳ǫ的非零元素表示异常值。<br>本论文中提到的是一个线性模型，可以表示为，</p><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321095837367.png" alt="image-20200321095837367"></p><p>其中G是不相交的组索引集的集合，s是指定特征尺寸上限的标量，<strong>s</strong>是一个向量。 Hs是稀疏学习中常用的稀疏假设空间，s表示组稀疏集； HG，s是唯一的稀疏集，它增强了选择的多样性。</p><p>最终的模型为：</p><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321100114244.png" alt="image-20200321100114244"></p><p>其中（5）式的前两项为正则项，往往较小。</p><p>由于指标函数不是连续的，所以可以通过凸连续代替函数进行近似。</p><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321100308050.png" alt="image-20200321100308050"></p><p>误差公式：</p><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321100542274.png" alt="image-20200321100542274"></p><p>将logistic loss近似只是函数，（7）可以写为：</p><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321100609808.png" alt="image-20200321100609808"></p><p>我们得到了异常值检测和特征选择的误差最小化公式：</p><p><img src="//tracywoo.cn/2020/03/21/基于AUC最大化的同时进行异常值检测和特征选择的PU分类/image-20200321100751890.png" alt="image-20200321100751890"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><strong>参数调整</strong></p><p>在没有规格的情况下，我们使用以下方式调整模型中的超参数（6）。在实验中，我们选择H = Hs。有四个超参数：α，β，t（离群上限）和s（特征稀疏度）。由于仅使用α和β来限制w和the的大小，因此性能对这两个超参数不太敏感。因此在实践中，它们被选择为较小的值，例如α=β= 0.001。 t和s对性能很重要。它们的作用与L1范数稀疏正则化的权重相同，但它们是离散的，并且更容易调整。我们使用较小的整数初始化t和s（例如t = 0且s =特征总数的5％），并以贪婪的方式增加每个超参数的值，直到训练集的性能停止改善。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;PU学习的应用十分广泛，可以应用于卫生保健、文本分类以及生物信息领域，对于复杂的数据，一个鲁棒的PU学习算法是十分必要的，因此本文提出了公式
      
    
    </summary>
    
    
    
      <category term="MOEA" scheme="http://tracywoo.cn/tags/MOEA/"/>
    
  </entry>
  
  <entry>
    <title>NSGA2</title>
    <link href="http://tracywoo.cn/2020/03/18/NSGA2/"/>
    <id>http://tracywoo.cn/2020/03/18/NSGA2/</id>
    <published>2020-03-18T02:59:08.000Z</published>
    <updated>2020-03-18T04:24:58.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>NSGA2</strong>(Non-dominated sorting in genetic algorithm2)</p><p>相比较NSGA从以下三个部分进行了改善：保留了最优个体；不用设置共享参数；构造Pareto最优解集的时间复杂度有所降低。</p><p><strong>算法的时间开销由三部分组成</strong>：</p><p>构造分类子集(Non-dominated sort) O(r(2N)^2)</p><p>计算聚集距离</p><p>构造偏序集：个体之间的偏序<img src="//tracywoo.cn/2020/03/18/NSGA2/e6.PNG" alt="e6"> </p><h2 id="非支配集的构造方法"><a href="#非支配集的构造方法" class="headerlink" title="非支配集的构造方法"></a>非支配集的构造方法</h2><p>为每一个个体p设置两个性质：np记录p支配个体的数目，sp记录被p支配的个体的集合。</p><p><img src="//tracywoo.cn/2020/03/18/NSGA2/e1.PNG" alt="e1"></p><p>通过一个二重循环计算每个个体的np和sp，则</p><p><img src="//tracywoo.cn/2020/03/18/NSGA2/e2.PNG" alt="e2"></p><p><img src="//tracywoo.cn/2020/03/18/NSGA2/e3.PNG" alt="e3"></p><p><strong>构造非支配集的过程</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">nondominated-<span class="built_in">sort</span>(P)</span><br><span class="line">任意Pop中的个体p，np=0,Sp为空 //初始化</span><br><span class="line"><span class="keyword">for</span> p in Pop</span><br><span class="line"><span class="keyword">for</span> q in Pop</span><br><span class="line">    <span class="keyword">if</span> p支配q</span><br><span class="line">         Sp = [ Sp q ];</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> q支配p</span><br><span class="line">        np = np + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">end</span> <span class="keyword">if</span></span><br><span class="line">     <span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">     <span class="keyword">if</span> np = <span class="number">0</span></span><br><span class="line">     P1 = [P1 , np];</span><br><span class="line">     <span class="keyword">end</span> <span class="keyword">if</span></span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line"><span class="built_in">i</span> = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span>(!Pi)</span><br><span class="line">H = [];</span><br><span class="line"><span class="keyword">for</span> p in Pi</span><br><span class="line"><span class="keyword">for</span> q in Sp</span><br><span class="line">nq = nq - <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span> nq == <span class="number">0</span></span><br><span class="line">H = [H q];</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line"><span class="built_in">i</span> = <span class="built_in">i</span> + <span class="number">1</span>;</span><br><span class="line">Pi = H;</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">while</span></span><br></pre></td></tr></table></figure><h2 id="保持解群体分布性和多样性的方法"><a href="#保持解群体分布性和多样性的方法" class="headerlink" title="保持解群体分布性和多样性的方法"></a>保持解群体分布性和多样性的方法</h2><p>产生新群体时，通常将优秀且聚集度比较小的个体保留参与下一代进化，聚集密度小的个体聚集距离反而大。</p><p>本算法中引入了拥挤距离。</p><p>对于一个双目标的算法</p><p><img src="//tracywoo.cn/2020/03/18/NSGA2/e4.PNG" alt="e4"></p><p><img src="//tracywoo.cn/2020/03/18/NSGA2/e5.jpg" alt="e5"></p><p><strong>计算个体之间的拥挤距离</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">crowding-distance-assignment(P)</span><br><span class="line">N = |P|</span><br><span class="line"><span class="keyword">for</span> each <span class="built_in">i</span></span><br><span class="line">P_distance = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> each objective m</span><br><span class="line">P = <span class="built_in">sort</span>(P,m);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">2</span> to (N<span class="number">-1</span>)</span><br><span class="line">P[<span class="built_in">i</span>]_distance = P[<span class="built_in">i</span>]_distance + (P[<span class="built_in">i</span>+<span class="number">1</span>]_distance.m-P[<span class="built_in">i</span><span class="number">-1</span>]_distance.m)</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">P[<span class="number">0</span>]_distance = P[N]_distance = 无穷;</span><br></pre></td></tr></table></figure><p><strong>NSGA2</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">初始化随机产生一个初始群体 P_0</span><br><span class="line">Q_0 = make_new_pop(P_0);</span><br><span class="line">t = <span class="number">0</span>;</span><br><span class="line">R_t = R_t + Q_t;</span><br><span class="line">F = nondominated-<span class="built_in">sort</span>(R_t);</span><br><span class="line">P_t+<span class="number">1</span> = [] and <span class="built_in">i</span> = <span class="number">1</span>;</span><br><span class="line">Until(|P_t+<span class="number">1</span>|+|F_i|&lt;=N)</span><br><span class="line">P_t=<span class="number">1</span> = p_t+<span class="number">1</span> + F_i;</span><br><span class="line"><span class="built_in">i</span> = <span class="built_in">i</span> + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">crowding-distance-assignment(F_i);</span><br><span class="line"><span class="built_in">sort</span>(F_i,&gt;);</span><br><span class="line">P_t+<span class="number">1</span> = P_t+<span class="number">1</span> + F_i(<span class="number">1</span>:N-|P_t+<span class="number">1</span>|);</span><br><span class="line">Q_t+<span class="number">1</span> = make_new_pop(P_t+<span class="number">1</span>);</span><br><span class="line">t = t + <span class="number">1</span>;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;NSGA2&lt;/strong&gt;(Non-dominated sorting in genetic algorithm2)&lt;/p&gt;
&lt;p&gt;相比较NSGA从以下三个部分进行了改善：保留了最优个体；不用设置共享参数；构造Pareto最优解集的时间复杂度有所降低。&lt;/
      
    
    </summary>
    
    
    
      <category term="MOEA" scheme="http://tracywoo.cn/tags/MOEA/"/>
    
  </entry>
  
</feed>
